## 人工智能概述  
### 机器学习与人工智能、深度学习  
三者之间是包含与被包含的关系  

- 人工智能：最大的概念  
    最开始的时候是为了实现自动的下棋，那个时候就是人工智能了  
    在 1958 年，有最开始的人工智能的会议。
- 机器学习：实际上这个东西在上个世纪八十年代上就得到了广泛的应用
- 深度学习：在图像识别中取得了不错的效果
    - 应用在挖掘数据
    - 应用在图像的识别
    - 应用在自然语言的处理——翻译，还有很多的聊天的人工智能

### 什么是机器学习  
#### 定义  
机器学习是从**数据**中自动分析获得**模型**，并利用模型对**未知的数据进行预测**  

#### 解释  
利用以往的规律进行学习  

> [!tip] 例子  
> 比如使用人工智能识别猫和狗的照片，就是使用大量的图片进行训练得到的  
> 还有房屋价格的预测等等  

但是这些历史数据应该是怎么样的？

#### 数据集的组成  
- 结构：特征值+目标值

> [!tip] 注  
> 对于每一行数据我们可以称之为样本  
> 有些数据集可以没有目标值  

### 机器学习算法分类  
要使得机器有着识别猫和狗的能力，应该从机器中进行学习  
我们需要的也是特征值和目标值  
这里的目标值就是猫？还是狗  

---
所以这里的目标值是类别，属于**分类问题**  
但是对于房屋价格的预测，最后的目标值是一种连续型的数据，属于**回归问题**  
当我们遇到的数据集中**没有目标值**时，称为**无监督学习**  

- 监督学习
    - 分类问题
    - 回归问题
- 无监督学习

---

> [!quote] 例子  
> 预测气温是多少度，是回归问题  
> 预测明天的天气，是分类问题  
> 人脸的年龄预测：看怎么定义，可以是回归/分类  
> 人脸的识别：分类  

#### 机器学习算法的分类  
- 监督学习：
    - 分类：K-近邻、贝叶斯、决策树与随机森林、逻辑回归
    - 回归：线性回归，岭回归
- 无监督学习
    - 聚类 k-means

### 机器学习的开发流程  
!!! Tip "流程"  
    1. 获取数据  
    2. 数据处理  
    3. 特征工程  
    4. 机器学习算法训练-模型  
    5. 模型评估  
![](png/Pasted%20image%2020250731180031.png)  

### 学习框架和资料介绍  
重点的问题：  

1. 算法是核心，数据和计算是基础
2. 找准定位  
大部分复杂模型的算法设计都是算法工程师在做，实战类的书籍  

---
书籍：  
- 机器学习-周志华
- 统计学习方法-李航
- 深度学习-"花书"

#### 常见的深度学习的框架  
一般是 pytorch 和 TF 使用的多一点  

## 特征工程  
### 数据集  
#### 可用数据集   
公司内部：百度之类的  
数据接口：花钱  
数据集  

---
所以在学习阶段我们会用到哪些数据集？  
1. Sklearn
2. Kaggle
3. Ucl  
我们现在主要介绍一下 sklearn  
- Python 语言的机器学习工具
- 包括很多的知名的机器学习算法的实现
- 易于上手
- 安装 pip install Scikit-learn (在 cmd 中运行安装)（使用的是 pycharm 的话在里面的终端安装即可）
- 安装好之后直接在 python 中好像可以直接使用  
    如下的代码块

```python
# 1. 导入数据集模块
from sklearn import datasets

# 2. 加载经典数据集（以鸢尾花数据集为例）
iris = datasets.load_iris()  # 这是一个内置的小数据集，无需下载

# 3. 查看数据集内容
print("特征数据（前5行）：\n", iris.data[:5])  # 数据集的特征（输入变量）
print("\n标签数据：\n", iris.target)  # 数据集的标签（输出变量/分类结果）
print("\n特征名称：\n", iris.feature_names)  # 每个特征的名称
print("\n标签名称：\n", iris.target_names)  # 每个标签对应的类别名称
print("\n数据集描述：\n", iris.DESCR)  # 数据集的详细说明
```

> [!tip] 介绍  
> 他的文档是非常完善的  
> 可以直接在文档中进行相关的学习  
> [网址](https://scikit-learn.org.cn/)  

### 如何使用这个数据集  
#### API 介绍  
- sklearn. datasets
    - 加载获取流行数据集
    - datasets. load_\*()
        - 获取**小规模**的数据集
    - datasets. fetch_\*(data_home=None)
        - 获取**大量**的数据集，需要从网络上下载

#### 小数据集  
- datasets. load_iris ()  
    加载并返回鸢尾花数据集
- datasets. load_boston ()  
    加载并返回波士顿房价数据集

#### 大数据集  
- sklearn.datasets.fetch_20newsgroups(data_home=None,subset='train')
    - 其中的 train 为训练的数据集，还可以为test 为测试的数据集，所有的就是 all，一般选择的是这个

#### 数据集返回值  
上述的返回的数据类型都是字典类型，可以使用 `[""]` 的方式获取，或者使用`.` 的方式获取  

- data：特征数据数组
- targets：标签数组
- DERCR：数据描述
- feature_names: 特征名
- target_names：标签名

```python
from sklearn.datasets import load_iris
# 获取鸢尾花数据集
iris = load_iris()
print("鸢尾花数据集的返回值: \n", iris)
print("返回值是一个继承自字典的Bench")
print("鸢尾花的特征值:\n", iris["data"])
print("鸢尾花的目标值: \n", iris.target)
print("鸢尾花特征的名字: \n", iris.feature_names)
print("鸢尾花目标值的名字: \n", iris.target_names)
print("鸢尾花的描述: \n", iris.DESCR)
```

首先使用下面的代码**输出所有的数据**

```python
from sklearn.datasets import load_iris  
# 获取鸢尾花数据集  
  
def datasets_demo():  
    iris = load_iris()  
    print("鸢尾花数据集：\n",iris)  
    return None  
datasets_demo()
```
输出的结果为  ：  

![](png/Pasted%20image%2020250822004756.png)  

![](png/Pasted%20image%2020250822004825.png)  

---

使用  
```Python
print("查看数据集描述：\n",iris["DESCR"])
```

输出了数据集的**特征**：  

![](png/Pasted%20image%2020250822005149.png)  

> [!tip] 输出的特征  
> sepal length:   4.3  7.9   5.84   0.83    0.7826  
> sepal width:    2.0  4.4   3.05   0.43   -0.4194  
> petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)  
> petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)  

`print("查看特征值的名字,\n",iris.feature_names)`  
输出的结果就是：` ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']`  

#### 数据集的划分  
机器学习的数据集一般会分为两个部分：  
- 训练数据：用于训练、构件模型
- 测试数据：在模型检验时使用，用于评估**模型是否有效**  
划分的比例：  
- 训练集：70.75.80
- 测试集：上面的相减

---
!!! tip "划分"  
    - sklearn.model_selection.train_test_split(arrays, \*options)     
        - x 数据集的特征值  
        - y 数据集的标签值  
        - test_size：测试集的大小，一般为 float  
        - random_state：随机数种子，不同的种子生成不同的随机采样数，相同的种子采样结果相同  
        - return 训练集特征值，测试集特征值，训练集目标值，测试集目标值
```Python
from sklearn.datasets import load_iris  
from sklearn.model_selection import train_test_split  
# 获取鸢尾花数据集  
  
def datasets_demo():  
    iris = load_iris()  
    # print("鸢尾花数据集：\n",iris)  
    # print("查看数据集描述：\n",iris["DESCR"])  
    # print("查看特征值的名字,\n",iris.feature_names)  
    # 数据集划分  
    x_train,x_test,y_train,y_test=train_test_split(iris.data,iris.target,test_size=0.2)# 默认的情况下是0.25  
    print("训练集的特征值为\n",x_train,x_train.shape)# 后一个是输出数据集的行数
```

通过这种方式输出，获取测试集和训练集的特征值和标签值（就是字典的标签和特征）  

### 特征工程的介绍  
使用的**算法，特征工程**导致的效果的差别  

---
**数据和特征**决定了机器学习的上限，而**模型和算法**只是逼近这个上限  

#### 什么是特征工程  

> [!tip] 什么  
> 使用专业的背景知识和技巧处理数据，使得特征能在机器学习中更好地作用。  

使用的是：  
- sklearn：特征工程
- pandas：数据清洗、数据处理  
    包括：  
        特征抽取  
        特征预处理  
        特征降维

#### 特征提取  

> [!tip] 什么事特征提取呢  
> 有一个数据集  
> 使用机器学习算法——统计方法——数学公式（但是数学公式不能处理字符串）  
> 所以需要将**数据集中的字符串转换为数值的类型**  

---
所以需要的转换：
- 字典特征提取（特征离散化）
- 文本特征提取
- 图像特征提取（深度学习介绍的）

#### 字典特征提取  
**对字典数据进行特征值化**  

- `sklearn.feature_extraction.DictVectorizer(sparse=True,...)`
    - `vector`：（数学中向量，物理中的矢量）  
        使用矩阵进行存储（matrix），也就是使用**二维数组**进行存储  
        向量是一维数组  
        所以每一个样本就是一个向量，n 个样本就形成了举证
    - 父类：转换器类  
        DictVectorizer. fit_transform (X)：字典或者是包含字典的迭代器返回值，返回 spare 矩阵

> [!tip] 实际上  
> 就是将字典中的类别转换为 one-hot 编码

!!! tip "使用的方法"  
    1. 引入下面的：`from sklearn.feature_extraction import DictVectorizer`    抽取的代码  
    2. 加上下面的：
       
    ```Python
    def dict_demo():  
        data = [  
            {'city': '北京', 'temperature': 100},  
            {'city': '上海', 'temperature': 60},  
            {'city': '深圳', 'temperature': 30}  
        ]  
    #     1、实例化一个转化器类  
        transfer=DictVectorizer()  
        # 2、 调用fit_transform()  
        data_new=transfer.fit_transform(data)  
        print("data_new:",data_new)  
    dict_demo()
    ```  

    3.  最后的输出结果为：  
        ![](png/Pasted%20image%2020250824230413.png)  

---

为什么会出现这种结果？  
    这个我们就需要分析一下了  
    主要是因为上面有着**默认**的 `sparse=True,...`  （在什么都不加的时候）  
    所以将上面的代码中的一句改为 `transfer=DictVectorizer(sparse=False)` 这种，输出结果就对了  
![](png/Pasted%20image%2020250825002811.png)  

---
**上面的两种形式是等价的**  
![](png/Pasted%20image%2020250824230413.png)  
这个是稀疏矩阵，两行是一组  

> [!question] 特点  
> 将非零值使用位置来表示出来  
> 比如前两行，1 的位置是（1,0），100 的位置是（0,3）  
> 这种的目的是什么呢  
>     种类非常多时，0 就会非常多，所以使用这种方法可以**节省内存**  

通过这样 `print("特征名字：\n",transfer.get_feature_names_out())` 输出特征名字  
输出的结果为：  
    特征名字：  
     ['city=上海' 'city=北京' 'city=深圳' 'temperature']

所以对于字典中的特征都进行了 one-hot 编码  

!!! tip "应用场景"  
    1. 数据集中的类别特征比较多时  
        1. 将数据集的特征——>字典类型  
        2. DictVectorizer 类型转换  
    2. 本身拿到的数据的类型就是字典类型

#### 文本特征提取  
- 将单词作为特征来分类是比较合理的
- 句子、单词、短语、字母中  
    这些中使用单词作为特征  
    特征：特征词

---
!!! tip "方法"
1. `sklearn.feature_extraction.text.CountVectorizer(stop_words=[])`  
    1. 返回词频矩阵
2. `CountVectorizer.fit_transform(X)`，X：文本或者是包含文本字符串的可迭代对象，返回值：返回 sparse 矩阵
3. `CountVectorizer.inverse_transform(X)`：X 为 array 数组或者是 sparse 矩阵，返回值为：转换之前的数据格

---
举例：  
首先引入下面的：`from sklearn.feature_extraction.text import CountVectorizer`  
其次加上下面的代码：  
```Python
def count_demo():  
    """  
    文本特征抽取：countvectorizer  
    :return:    """    data = ["life is short,i like like python",  
     "life is too long,i dislike python"]  
  
    # 1.实例化一个转换器类  
    transfer=CountVectorizer()  
    # 2.调用fit_transform  
    data_new=transfer.fit_transform(data)  
    print("data_new:\n",data_new.toarray())  
    print("特征名字：\n",transfer.get_feature_names_out())  
    return None  
count_demo()
```
最后的结果为  
     data_new:  
     [[0 1 1 2 0 1 1 0]  
     [1 1 1 0 1 1 0 1]]  
    特征名字：  
     ['dislike' 'is' 'life' 'like' 'long' 'python' 'short' 'too']  

---
1. CountVectorizer() 的方法  
    统计每个样本特征出现的次数（不统计单个的字母和标点符号）  
    stop_words 停用的词，可以将觉得没有用处的词以列表的形式传进去  
    还有专门的停用词表  
    例如 `transfer=CountVectorizer(stop_words=["is","to"])`，最后的输出结果就是没有 is 和 to 了

> [!tip] 出现的是中文的文本时怎么办  
> 就直接将里面的英文改为中文  
> 但是最后的输出结果为  
>  [[0 1]  
>  [1 0]]  
> 特征名字：  
> ['天安门上太阳升' '我爱北京天安门']  
> 所以要有需要的效果的话，需要使用**空格隔开对应的词语**  
> data = ["我 爱 北京 天安门",    
> "天安门 上 太阳 升"]

最后的输出结果变成了：  
![](png/Pasted%20image%2020250825193939.png)  
还是没有管单个的词语  
用于分词的可以使用**结巴分词**，或者之后使用一些专业的库进行分词  

#### 文本特征提取的其他方法  
首先使用清华镜像网站安装 jieba（在内部的终端中）`pip install jieba -i https://pypi.tuna.tsinghua.edu.cn/simple/`  ，并在开头进行引用  

---
首先看一下这个的功能  
```Python
import jieba #调用
def cut_word(text):  
    """  
    进行中文分词："我爱北京天安门"->"我 爱 北京 天安门"  
    :param text:    :return:    """    a=" ".join(list(jieba.cut(text)))#此时是生成器的形式，要强制转换为列表的形式  
    print(a)  
    print(type(a))  
    return a
```
上面函数的输出结果为  

> [!tip] 结果  
> 我 爱 北京 天安门  
> <class 'str'>  
> 其中的" ".join(list(jieba.cut(text))) 的意思是  
> 1. `jieba.cut(text)`：使用 jieba 库的 `cut` 方法对文本 `text` 进行分词，返回一个可迭代的分词结果（生成器）  
> 2. `list(jieba.cut(text))`：将分词结果转换为列表，每个元素是一个词语。  
> 3. `" ".join(...)`：用空格作为分隔符，将列表中的所有词语拼接成一个完整的字符串。

---
之后就可以自动的中文分词，中文  
```Python
def cout_chinese_demo():  
    """  
    中文文本的特征抽取  
    :return:    """    data=["一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。",  
          "我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。",  
          "如果只用一种方式了解某样事物，你就不会真正了解它。解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。"]  
    # 1、将中文文本进行分词  
    data_new=[]  
    for sent in data:  
        data_new.append(cut_word(sent))  
    # 2、实例化一个转换器类  
    transfer=CountVectorizer(stop_words=["一种","所以"])  
    # 3、调用fit_transfrom，输出的结果为矩阵  
    data_f=transfer.fit_transform(data_new)  
    print("data_new:\n",data_f.toarray())  
    print("特征名字：\n",transfer.get_feature_names_out())  
    return None  
# cut_word("我爱北京天安门")  
cout_chinese_demo()
```
实际上就是多了一步，除此之外还多了一个函数  
最后的输出结果为  
![](png/Pasted%20image%2020250825230349.png)  

但是这个输出结果的顺序排的不好，因为我们会想把出现次数高的排在前面  

### 新的提取方法——TF-idf 文本特征提取  
**关键词**：在某一个类别文章中，出现的次数很多，但是在其他类别的文章中出现的很少  

- 主要的思想：某个词或短语在一篇文章中出现的概率高，并且在其他文章中很少出现，则认为此词具有很好的类别区分能力，适合用来分类
- 作用：用于评估一字词对于某一文件集或者一个语料库中的其中一份文件的重要程度

---
- TfdfVectorize  
    TF-IDF——重要程度  
    例如：**两个词**：“经济”，“非常”  
    一共 1000 篇文章  
        其中 100 篇文章有“**非常**”  
        10 篇文章——“经济”  
    两篇文章：  
        文章 A（100 词）：10 次“经济”：0.2  
            TF：10/100=0.1  
            idf=lg 1000/10=2  
        文章 B（100 词）：10 次“非常” : 0.1  
            TF: 10/100=0.1  
            idf: log 10 (1000/100)=1

公式：  

- 词频（TF）指的是给定的词语在该文件中出现的频率
- 逆向文档频率（idf）：一个词语普遍重要性的度量——由**总文件数目除以包含该词语文件的数目，再将得到的商取以 10 为底的对数**  
最后的计算公式：（两个数值相乘即可）  

$$
\mathrm{tfidf_{i,j}=tf_{i,j}\times idf_i}
$$

最后得到的结果就是**重要程度**  

#### API  
- `sklearn.feature extraction.text.TfidfVectorizer(stop words=None,...)` 实例化
- 返回词的权重矩阵
    - `TfidfVectorizer.fitttransform(X)`  
        x: 文本或者包含文本的字符串的可迭代对象  
        返回值：返回 sparse 矩阵（使用 toarray 变为数组的形式）

加上后面的`from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer`  
再有后面的代码：  
```Python
def tfidf_demo():  
    """  
    使用tf-idf的方法进行文本特征提取  
    :return:    """    data=["一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。",  
          "我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。",  
          "如果只用一种方式了解某样事物，你就不会真正了解它。解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。"]  
    # 将中文文本进行分词  
    data_new=[]  
    for sent in data:  
        data_new.append(cut_word(sent))  
    # 实例化一个转换器类,新的转换器  
    transfer=TfidfVectorizer(stop_words=["一种","所以"])  #主要改这个地方
    # 调用fit_transfrom  
    data_f=transfer.fit_transform(data_new)  
    print("data_new:\n",data_f.toarray())  
    print("特征名字：\n",transfer.get_feature_names_out())  
    return None
```

最后的输出结果  
![](png/Pasted%20image%2020250826155338.png)

### 特征预处理  
#### 什么是特征预处理  
通过一些转换函数将特征数据转换成更加**适合算法模型**的特征数据过程  

- 包含内容  
    数值型数据的无量纲化
    - 归一化
    - 标准化
- 特征预处理 API：`sklearn.preprocessing`

---
我们为什么要进行归一化？  
使用距离公式计算两个样本之间的距离，但是由于某一特征的数值过大，导致距离取决于数值大的特征，但是我们现在认为特征是同等重要的  
所以我们用到无量纲化，使得**特征变为同一规格**  

#### 归一化  
**定义**：  
    通过对原始数据进行变换把数据映射到默认的[0,1]之间  
**定义**：  

$$  
X^{\prime}=\frac{x-min}{max-min}\quad X^{\prime\prime}=X^{\prime}*(mx-mi)+mi  
$$

> [!tip] 说明  
> 作用于每一列，max 为一列的最大值，min 为一列的最小值，$x^{''}$ 为最终的结果  
> mx, mi 分别为指定区间，默认 mx 为 1，mi 为 0  

实际上就是将每一列的数值放缩平移到数轴上的[0,1]区间  

**方法**：  

- `sklearn.preprocessing.MinMaxScaler (feature range=(0,1)...)`
    - `MinMaxScalar.fit transform(X)`
        - x: numpy array 格式的数据 [n_samples, n_features]（就是一个数组）
    - 返回值：转换后的形状形同的 array

---
例子：  

!!! tip "方法"  
    - 首先使用 panda 函数获取数据：`data=pd.read_csv("dating.txt",sep='\t')`，panda 也是需要安装的，其中的 dating. text 文件必须在同样的目录下  
    - 实例化转化器类：  
        - 头文件：`from sklearn.preprocessing import MinMaxScaler`  
        - `transfer=MinMaxScaler(feature_range=(2,3))`  
    - 调用转化：  
        - `data_new=transfer.fit_transform(data)`  
        最后的代码：
        
        ```Python
        def minmax_demo():  
            """  
            归一化  
            :return:    """#     1. 获取数据  
            data=pd.read_csv("dating.txt",sep='\t')  
            print("data:\n",data)  
            data=data.iloc[:,:3]  
        # 2.实例化一个转换器类  
            transfer=MinMaxScaler(feature_range=(2,3))  
          
        # 3. 调用fit_transform转化  
            data_new=transfer.fit_transform(data)  
            print("data_new:\n",data_new)  
            return None
        ```

!!! tip "输出结果"      
    data_new:  
     [[2.43582641 2.58819286 2.53237967]  
     [2.         2.48794044 3.        ]  
     [2.19067405 2.         2.43571351]  
     [3.         3.         2.19139157]  
     [2.3933518  2.01947089 2.        ]]  

---
分析一下上面的有什么**缺点**：  
如果有异常值的话，一般是最大值或者是最小值产生异常  
而我们归一化的时候使用的就是最大值和最小值，非常容易受到异常值的影响  
**所以这种方法的鲁棒性非常差，只适合传统精确小数据的场景**  

#### 标准化  
- 定义：  
    对原始数据变换为均值为 0，标准差为 1 的范围内
- 公式

    $$
        X^{\prime}=\frac{x-\mathrm{mean}}{\sigma}
    $$

    mean 为平均值，$\sigma$ 为标准差

所以我们现在再看一下，异常值在有大量数据的情况下不会使标准差产生太大的影响  

---
- `sklearn.preprocessing.StandardScaler( )`
    - 处理之后，每列数据都聚焦在均值为 0. 标准差为 1 的
    - `StandardScaler.fit_transform(X)`
        - X：numpy array 格式的数据（数组形式的）
    - 返回值：转换后形状形同的 array

!!! tip "步骤"
    - 在头文件中加上：`from sklearn.preprocessing import MinMaxScaler,StandardScaler`    
    - 加上下面的代码：  

    ```Python
    def stand_demo():  
        """  
        标准化  
        :return:    """    #     1. 获取数据  
        data = pd.read_csv("dating.txt", sep='\t')  
        print("data:\n", data)  
        data = data.iloc[:, :3]  
        # 2.实例化一个转换器类  
        transfer = StandardScaler()  
      
        # 3. 调用fit_transform转化  
        data_new = transfer.fit_transform(data)  
        print("data_new:\n", data_new)  
        return None  
    stand_demo()
    ```
    - 最后的输出结果为  
        ![](png/Pasted%20image%2020250826180723.png)  

