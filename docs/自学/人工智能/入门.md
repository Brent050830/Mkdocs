## 人工智能概述  
### 机器学习与人工智能、深度学习  
三者之间是包含与被包含的关系  

- 人工智能：最大的概念  
    最开始的时候是为了实现自动的下棋，那个时候就是人工智能了  
    在 1958 年，有最开始的人工智能的会议。
- 机器学习：实际上这个东西在上个世纪八十年代上就得到了广泛的应用
- 深度学习：在图像识别中取得了不错的效果
    - 应用在挖掘数据
    - 应用在图像的识别
    - 应用在自然语言的处理——翻译，还有很多的聊天的人工智能

### 什么是机器学习  
#### 定义  
机器学习是从**数据**中自动分析获得**模型**，并利用模型对**未知的数据进行预测**  

#### 解释  
利用以往的规律进行学习  

> [!tip] 例子  
> 比如使用人工智能识别猫和狗的照片，就是使用大量的图片进行训练得到的  
> 还有房屋价格的预测等等  

但是这些历史数据应该是怎么样的？

#### 数据集的组成  
- 结构：特征值+目标值

> [!tip] 注  
> 对于每一行数据我们可以称之为样本  
> 有些数据集可以没有目标值  

### 机器学习算法分类  
要使得机器有着识别猫和狗的能力，应该从机器中进行学习  
我们需要的也是特征值和目标值  
这里的目标值就是猫？还是狗  

---
所以这里的目标值是类别，属于**分类问题**  
但是对于房屋价格的预测，最后的目标值是一种连续型的数据，属于**回归问题**  
当我们遇到的数据集中**没有目标值**时，称为**无监督学习**  

- 监督学习
    - 分类问题
    - 回归问题
- 无监督学习

---

> [!quote] 例子  
> 预测气温是多少度，是回归问题  
> 预测明天的天气，是分类问题  
> 人脸的年龄预测：看怎么定义，可以是回归/分类  
> 人脸的识别：分类  

#### 机器学习算法的分类  
- 监督学习：
    - 分类：K-近邻、贝叶斯、决策树与随机森林、逻辑回归
    - 回归：线性回归，岭回归
- 无监督学习
    - 聚类 k-means

### 机器学习的开发流程  
!!! Tip "流程"  
    1. 获取数据  
    2. 数据处理  
    3. 特征工程  
    4. 机器学习算法训练-模型  
    5. 模型评估  
![](png/Pasted%20image%2020250731180031.png)  

### 学习框架和资料介绍  
重点的问题：  

1. 算法是核心，数据和计算是基础
2. 找准定位  
大部分复杂模型的算法设计都是算法工程师在做，实战类的书籍  

---
书籍：  
- 机器学习-周志华
- 统计学习方法-李航
- 深度学习-"花书"

#### 常见的深度学习的框架  
一般是 pytorch 和 TF 使用的多一点  

## 特征工程  
### 数据集  
#### 可用数据集   
公司内部：百度之类的  
数据接口：花钱  
数据集  

---
所以在学习阶段我们会用到哪些数据集？  
1. Sklearn
2. Kaggle
3. Ucl  
我们现在主要介绍一下 sklearn  
- Python 语言的机器学习工具
- 包括很多的知名的机器学习算法的实现
- 易于上手
- 安装 pip install Scikit-learn (在 cmd 中运行安装)（使用的是 pycharm 的话在里面的终端安装即可）
- 安装好之后直接在 python 中好像可以直接使用  
    如下的代码块

```python
# 1. 导入数据集模块
from sklearn import datasets

# 2. 加载经典数据集（以鸢尾花数据集为例）
iris = datasets.load_iris()  # 这是一个内置的小数据集，无需下载

# 3. 查看数据集内容
print("特征数据（前5行）：\n", iris.data[:5])  # 数据集的特征（输入变量）
print("\n标签数据：\n", iris.target)  # 数据集的标签（输出变量/分类结果）
print("\n特征名称：\n", iris.feature_names)  # 每个特征的名称
print("\n标签名称：\n", iris.target_names)  # 每个标签对应的类别名称
print("\n数据集描述：\n", iris.DESCR)  # 数据集的详细说明
```

> [!tip] 介绍  
> 他的文档是非常完善的  
> 可以直接在文档中进行相关的学习  
> [网址](https://scikit-learn.org.cn/)  

### 如何使用这个数据集  
#### API 介绍  
- sklearn. datasets
    - 加载获取流行数据集
    - datasets. load_\*()
        - 获取**小规模**的数据集
    - datasets. fetch_\*(data_home=None)
        - 获取**大量**的数据集，需要从网络上下载

#### 小数据集  
- datasets. load_iris ()  
    加载并返回鸢尾花数据集
- datasets. load_boston ()  
    加载并返回波士顿房价数据集

#### 大数据集  
- sklearn.datasets.fetch_20newsgroups(data_home=None,subset='train')
    - 其中的 train 为训练的数据集，还可以为test 为测试的数据集，所有的就是 all，一般选择的是这个

#### 数据集返回值  
上述的返回的数据类型都是字典类型，可以使用 `[""]` 的方式获取，或者使用`.` 的方式获取  

- data：特征数据数组
- targets：标签数组
- DERCR：数据描述
- feature_names: 特征名
- target_names：标签名

```python
from sklearn.datasets import load_iris
# 获取鸢尾花数据集
iris = load_iris()
print("鸢尾花数据集的返回值: \n", iris)
print("返回值是一个继承自字典的Bench")
print("鸢尾花的特征值:\n", iris["data"])
print("鸢尾花的目标值: \n", iris.target)
print("鸢尾花特征的名字: \n", iris.feature_names)
print("鸢尾花目标值的名字: \n", iris.target_names)
print("鸢尾花的描述: \n", iris.DESCR)
```

首先使用下面的代码**输出所有的数据**

```python
from sklearn.datasets import load_iris  
# 获取鸢尾花数据集  
  
def datasets_demo():  
    iris = load_iris()  
    print("鸢尾花数据集：\n",iris)  
    return None  
datasets_demo()
```
输出的结果为  ：  

![](png/Pasted%20image%2020250822004756.png)  

![](png/Pasted%20image%2020250822004825.png)  

---

使用  
```Python
print("查看数据集描述：\n",iris["DESCR"])
```

输出了数据集的**特征**：  

![](png/Pasted%20image%2020250822005149.png)  

> [!tip] 输出的特征  
> sepal length:   4.3  7.9   5.84   0.83    0.7826  
> sepal width:    2.0  4.4   3.05   0.43   -0.4194  
> petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)  
> petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)  

`print("查看特征值的名字,\n",iris.feature_names)`  
输出的结果就是：` ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']`  

#### 数据集的划分  
机器学习的数据集一般会分为两个部分：  
- 训练数据：用于训练、构件模型
- 测试数据：在模型检验时使用，用于评估**模型是否有效**  
划分的比例：  
- 训练集：70.75.80
- 测试集：上面的相减

---
!!! tip "划分"  
    - sklearn.model_selection.train_test_split(arrays, \*options)     
        - x 数据集的特征值  
        - y 数据集的标签值  
        - test_size：测试集的大小，一般为 float  
        - random_state：随机数种子，不同的种子生成不同的随机采样数，相同的种子采样结果相同  
        - return 训练集特征值，测试集特征值，训练集目标值，测试集目标值
```Python
from sklearn.datasets import load_iris  
from sklearn.model_selection import train_test_split  
# 获取鸢尾花数据集  
  
def datasets_demo():  
    iris = load_iris()  
    # print("鸢尾花数据集：\n",iris)  
    # print("查看数据集描述：\n",iris["DESCR"])  
    # print("查看特征值的名字,\n",iris.feature_names)  
    # 数据集划分  
    x_train,x_test,y_train,y_test=train_test_split(iris.data,iris.target,test_size=0.2)# 默认的情况下是0.25  
    print("训练集的特征值为\n",x_train,x_train.shape)# 后一个是输出数据集的行数
```

通过这种方式输出，获取测试集和训练集的特征值和标签值（就是字典的标签和特征）  

### 特征工程的介绍  
使用的**算法，特征工程**导致的效果的差别  

---
**数据和特征**决定了机器学习的上限，而**模型和算法**只是逼近这个上限  

#### 什么是特征工程  

> [!tip] 什么  
> 使用专业的背景知识和技巧处理数据，使得特征能在机器学习中更好地作用。  

使用的是：  

- sklearn：特征工程
- pandas：数据清洗、数据处理  
    包括：  
        特征抽取  
        特征预处理  
        特征降维

#### 特征提取  

> [!tip] 什么事特征提取呢  
> 有一个数据集  
> 使用机器学习算法——统计方法——数学公式（但是数学公式不能处理字符串）  
> 所以需要将**数据集中的字符串转换为数值的类型**  

---
所以需要的转换：
- 字典特征提取（特征离散化）
- 文本特征提取
- 图像特征提取（深度学习介绍的）

#### 字典特征提取  
**对字典数据进行特征值化**  

- `sklearn.feature_extraction.DictVectorizer(sparse=True,...)`
    - `vector`：（数学中向量，物理中的矢量）  
        使用矩阵进行存储（matrix），也就是使用**二维数组**进行存储  
        向量是一维数组  
        所以每一个样本就是一个向量，n 个样本就形成了举证
    - 父类：转换器类  
        DictVectorizer. fit_transform (X)：字典或者是包含字典的迭代器返回值，返回 spare 矩阵

> [!tip] 实际上  
> 就是将字典中的类别转换为 one-hot 编码

!!! tip "使用的方法"  
    1. 引入下面的：`from sklearn.feature_extraction import DictVectorizer`    抽取的代码  
    2. 加上下面的：
       
    ```Python
    def dict_demo():  
        data = [  
            {'city': '北京', 'temperature': 100},  
            {'city': '上海', 'temperature': 60},  
            {'city': '深圳', 'temperature': 30}  
        ]  
    #     1、实例化一个转化器类  
        transfer=DictVectorizer()  
        # 2、 调用fit_transform()  
        data_new=transfer.fit_transform(data)  
        print("data_new:",data_new)  
    dict_demo()
    ```  

    3.  最后的输出结果为：  
        ![](png/Pasted%20image%2020250824230413.png)  

---

为什么会出现这种结果？  
    这个我们就需要分析一下了  
    主要是因为上面有着**默认**的 `sparse=True,...`  （在什么都不加的时候）  
    所以将上面的代码中的一句改为 `transfer=DictVectorizer(sparse=False)` 这种，输出结果就对了  
![](png/Pasted%20image%2020250825002811.png)  

---
**上面的两种形式是等价的**  
![](png/Pasted%20image%2020250824230413.png)  
这个是稀疏矩阵，两行是一组  

> [!question] 特点  
> 将非零值使用位置来表示出来  
> 比如前两行，1 的位置是（1,0），100 的位置是（0,3）  
> 这种的目的是什么呢  
>     种类非常多时，0 就会非常多，所以使用这种方法可以**节省内存**  

通过这样 `print("特征名字：\n",transfer.get_feature_names_out())` 输出特征名字  
输出的结果为：  
    特征名字：  
     ['city=上海' 'city=北京' 'city=深圳' 'temperature']

所以对于字典中的特征都进行了 one-hot 编码  

!!! tip "应用场景"  
    1. 数据集中的类别特征比较多时  
        1. 将数据集的特征——>字典类型  
        2. DictVectorizer 类型转换  
    2. 本身拿到的数据的类型就是字典类型

#### 文本特征提取  
- 将单词作为特征来分类是比较合理的
- 句子、单词、短语、字母中  
    这些中使用单词作为特征  
    特征：特征词

---
!!! tip "方法"
1. `sklearn.feature_extraction.text.CountVectorizer(stop_words=[])`  
    1. 返回词频矩阵
2. `CountVectorizer.fit_transform(X)`，X：文本或者是包含文本字符串的可迭代对象，返回值：返回 sparse 矩阵
3. `CountVectorizer.inverse_transform(X)`：X 为 array 数组或者是 sparse 矩阵，返回值为：转换之前的数据格

---
举例：  
首先引入下面的：`from sklearn.feature_extraction.text import CountVectorizer`  
其次加上下面的代码：  
```Python
def count_demo():  
    """  
    文本特征抽取：countvectorizer  
    :return:    """    data = ["life is short,i like like python",  
     "life is too long,i dislike python"]  
  
    # 1.实例化一个转换器类  
    transfer=CountVectorizer()  
    # 2.调用fit_transform  
    data_new=transfer.fit_transform(data)  
    print("data_new:\n",data_new.toarray())  
    print("特征名字：\n",transfer.get_feature_names_out())  
    return None  
count_demo()
```
最后的结果为  
     data_new:  
     [[0 1 1 2 0 1 1 0]  
     [1 1 1 0 1 1 0 1]]  
    特征名字：  
     ['dislike' 'is' 'life' 'like' 'long' 'python' 'short' 'too']  

---
1. CountVectorizer() 的方法  
    统计每个样本特征出现的次数（不统计单个的字母和标点符号）  
    stop_words 停用的词，可以将觉得没有用处的词以列表的形式传进去  
    还有专门的停用词表  
    例如 `transfer=CountVectorizer(stop_words=["is","to"])`，最后的输出结果就是没有 is 和 to 了

> [!tip] 出现的是中文的文本时怎么办  
> 就直接将里面的英文改为中文  
> 但是最后的输出结果为  
>  [[0 1]  
>  [1 0]]  
> 特征名字：  
> ['天安门上太阳升' '我爱北京天安门']  
> 所以要有需要的效果的话，需要使用**空格隔开对应的词语**  
> data = ["我 爱 北京 天安门",    
> "天安门 上 太阳 升"]

最后的输出结果变成了：  
![](png/Pasted%20image%2020250825193939.png)  
还是没有管单个的词语  
用于分词的可以使用**结巴分词**，或者之后使用一些专业的库进行分词  

#### 文本特征提取的其他方法  
首先使用清华镜像网站安装 jieba（在内部的终端中）`pip install jieba -i https://pypi.tuna.tsinghua.edu.cn/simple/`  ，并在开头进行引用  

---
首先看一下这个的功能  
```Python
import jieba #调用
def cut_word(text):  
    """  
    进行中文分词："我爱北京天安门"->"我 爱 北京 天安门"  
    :param text:    :return:    """    a=" ".join(list(jieba.cut(text)))#此时是生成器的形式，要强制转换为列表的形式  
    print(a)  
    print(type(a))  
    return a
```
上面函数的输出结果为  

> [!tip] 结果  
> 我 爱 北京 天安门  
> <class 'str'>  
> 其中的" ".join(list(jieba.cut(text))) 的意思是  
> 1. `jieba.cut(text)`：使用 jieba 库的 `cut` 方法对文本 `text` 进行分词，返回一个可迭代的分词结果（生成器）  
> 2. `list(jieba.cut(text))`：将分词结果转换为列表，每个元素是一个词语。  
> 3. `" ".join(...)`：用空格作为分隔符，将列表中的所有词语拼接成一个完整的字符串。

---
之后就可以自动的中文分词，中文  
```Python
def cout_chinese_demo():  
    """  
    中文文本的特征抽取  
    :return:    """    data=["一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。",  
          "我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。",  
          "如果只用一种方式了解某样事物，你就不会真正了解它。解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。"]  
    # 1、将中文文本进行分词  
    data_new=[]  
    for sent in data:  
        data_new.append(cut_word(sent))  
    # 2、实例化一个转换器类  
    transfer=CountVectorizer(stop_words=["一种","所以"])  
    # 3、调用fit_transfrom，输出的结果为矩阵  
    data_f=transfer.fit_transform(data_new)  
    print("data_new:\n",data_f.toarray())  
    print("特征名字：\n",transfer.get_feature_names_out())  
    return None  
# cut_word("我爱北京天安门")  
cout_chinese_demo()
```
实际上就是多了一步，除此之外还多了一个函数  
最后的输出结果为  
![](png/Pasted%20image%2020250825230349.png)  

但是这个输出结果的顺序排的不好，因为我们会想把出现次数高的排在前面  

### 新的提取方法——TF-idf 文本特征提取  
**关键词**：在某一个类别文章中，出现的次数很多，但是在其他类别的文章中出现的很少  

- 主要的思想：某个词或短语在一篇文章中出现的概率高，并且在其他文章中很少出现，则认为此词具有很好的类别区分能力，适合用来分类
- 作用：用于评估一字词对于某一文件集或者一个语料库中的其中一份文件的重要程度

---
- TfdfVectorize  
    TF-IDF——重要程度  
    例如：**两个词**：“经济”，“非常”  
    一共 1000 篇文章  
        其中 100 篇文章有“**非常**”  
        10 篇文章——“经济”  
    两篇文章：  
        文章 A（100 词）：10 次“经济”：0.2  
            TF：10/100=0.1  
            idf=lg 1000/10=2  
        文章 B（100 词）：10 次“非常” : 0.1  
            TF: 10/100=0.1  
            idf: log 10 (1000/100)=1

公式：  

- 词频（TF）指的是给定的词语在该文件中出现的频率
- 逆向文档频率（idf）：一个词语普遍重要性的度量——由**总文件数目除以包含该词语文件的数目，再将得到的商取以 10 为底的对数**  
最后的计算公式：（两个数值相乘即可）  

$$
\mathrm{tfidf_{i,j}=tf_{i,j}\times idf_i}
$$

最后得到的结果就是**重要程度**  

#### API  
- `sklearn.feature extraction.text.TfidfVectorizer(stop words=None,...)` 实例化
- 返回词的权重矩阵
    - `TfidfVectorizer.fitttransform(X)`  
        x: 文本或者包含文本的字符串的可迭代对象  
        返回值：返回 sparse 矩阵（使用 toarray 变为数组的形式）

加上后面的`from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer`  
再有后面的代码：  
```Python
def tfidf_demo():  
    """  
    使用tf-idf的方法进行文本特征提取  
    :return:    """    data=["一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。",  
          "我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。",  
          "如果只用一种方式了解某样事物，你就不会真正了解它。解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。"]  
    # 将中文文本进行分词  
    data_new=[]  
    for sent in data:  
        data_new.append(cut_word(sent))  
    # 实例化一个转换器类,新的转换器  
    transfer=TfidfVectorizer(stop_words=["一种","所以"])  #主要改这个地方
    # 调用fit_transfrom  
    data_f=transfer.fit_transform(data_new)  
    print("data_new:\n",data_f.toarray())  
    print("特征名字：\n",transfer.get_feature_names_out())  
    return None
```

最后的输出结果  
![](png/Pasted%20image%2020250826155338.png)

### 特征预处理  
#### 什么是特征预处理  
通过一些转换函数将特征数据转换成更加**适合算法模型**的特征数据过程  

- 包含内容  
    数值型数据的无量纲化
    - 归一化
    - 标准化
- 特征预处理 API：`sklearn.preprocessing`

---
我们为什么要进行归一化？  
使用距离公式计算两个样本之间的距离，但是由于某一特征的数值过大，导致距离取决于数值大的特征，但是我们现在认为特征是同等重要的  
所以我们用到无量纲化，使得**特征变为同一规格**  

#### 归一化  
**定义**：  
    通过对原始数据进行变换把数据映射到默认的[0,1]之间  
**定义**：  

$$  
X^{\prime}=\frac{x-min}{max-min}\quad X^{\prime\prime}=X^{\prime}*(mx-mi)+mi  
$$

> [!tip] 说明  
> 作用于每一列，max 为一列的最大值，min 为一列的最小值，$x^{''}$ 为最终的结果  
> mx, mi 分别为指定区间，默认 mx 为 1，mi 为 0  

实际上就是将每一列的数值放缩平移到数轴上的[0,1]区间  

**方法**：  

- `sklearn.preprocessing.MinMaxScaler (feature range=(0,1)...)`
    - `MinMaxScalar.fit transform(X)`
        - x: numpy array 格式的数据 [n_samples, n_features]（就是一个数组）
    - 返回值：转换后的形状形同的 array

---
例子：  

!!! tip "方法"  
    - 首先使用 panda 函数获取数据：`data=pd.read_csv("dating.txt",sep='\t')`，panda 也是需要安装的，其中的 dating. text 文件必须在同样的目录下  
    - 实例化转化器类：  
        - 头文件：`from sklearn.preprocessing import MinMaxScaler`  
        - `transfer=MinMaxScaler(feature_range=(2,3))`  
    - 调用转化：  
        - `data_new=transfer.fit_transform(data)`  
        最后的代码：
        
        ```Python
        def minmax_demo():  
            """  
            归一化  
            :return:    """#     1. 获取数据  
            data=pd.read_csv("dating.txt",sep='\t')  
            print("data:\n",data)  
            data=data.iloc[:,:3]  
        # 2.实例化一个转换器类  
            transfer=MinMaxScaler(feature_range=(2,3))  
          
        # 3. 调用fit_transform转化  
            data_new=transfer.fit_transform(data)  
            print("data_new:\n",data_new)  
            return None
        ```

!!! tip "输出结果"      
    data_new:  
     [[2.43582641 2.58819286 2.53237967]  
     [2.         2.48794044 3.        ]  
     [2.19067405 2.         2.43571351]  
     [3.         3.         2.19139157]  
     [2.3933518  2.01947089 2.        ]]  

---
分析一下上面的有什么**缺点**：  
如果有异常值的话，一般是最大值或者是最小值产生异常  
而我们归一化的时候使用的就是最大值和最小值，非常容易受到异常值的影响  
**所以这种方法的鲁棒性非常差，只适合传统精确小数据的场景**  

#### 标准化  
- 定义：  
    对原始数据变换为均值为 0，标准差为 1 的范围内
- 公式

    $$
        X^{\prime}=\frac{x-\mathrm{mean}}{\sigma}
    $$

    mean 为平均值，$\sigma$ 为标准差

所以我们现在再看一下，异常值在有大量数据的情况下不会使标准差产生太大的影响  

---
- `sklearn.preprocessing.StandardScaler( )`
    - 处理之后，每列数据都聚焦在均值为 0. 标准差为 1 的
    - `StandardScaler.fit_transform(X)`
        - X：numpy array 格式的数据（数组形式的）
    - 返回值：转换后形状形同的 array

!!! tip "步骤"  
    - 在头文件中加上：`from sklearn.preprocessing import MinMaxScaler,StandardScaler`    
    - 加上下面的代码：  

    ```Python
    def stand_demo():  
        """  
        标准化  
        :return:    """    #     1. 获取数据  
        data = pd.read_csv("dating.txt", sep='\t')  
        print("data:\n", data)  
        data = data.iloc[:, :3]  
        # 2.实例化一个转换器类  
        transfer = StandardScaler()  
      
        # 3. 调用fit_transform转化  
        data_new = transfer.fit_transform(data)  
        print("data_new:\n", data_new)  
        return None  
    stand_demo()
    ```  

    - 最后的输出结果为  
        ![](png/Pasted%20image%2020250826180723.png)  

### 特征降维  
#### 降维  
- 降维：字面上就是降低维度  
    对于数组来说：维数就是数组嵌套的数目  
    0 维就是标量（数字）  
    1 维向量  
    2 维矩阵  
    n 维
- 二维数组  
    此处的降维：**降低特征的个数**  
    得到一组**不相关**的主要变量的过程（极大线性无关组）
- 相关特征：
    - 相对湿度和降雨量相关
    - 等等，有很多数据是相关的，有很多冗余的数据

#### 降维的两种方法  
- 特征选择
- 主成分分析

#### 特征选择  
- 定义：  
    数据中包含冗余和相关变量，旨在从**原有的特征中找出主要的特征**
- 方法：
    - 过滤式（filter）：  
        方差选择法：低方差特征过滤  
        相关系数：衡量两个特征之间的相关性（相关程度）
    - 嵌入式：  
        决策树——第二天的内容  
        正则化——第三天的内容  
        深度学习——第五天的内容

#### 过滤式  
##### 低方差特征过滤  
删除低方差的一些特征  

- API：
    - `sklearn.feature_selection.VarianceThreshold(threshold=0.0)`
        - 删除所有的低方差特性（阈值就是后面括号内的值，低于那个的都删掉，默认为 0）
        - `{Variance.fit transform(X)`  
            x: 为数组  
            默认的返回值为所有非 0 的方差特征，即删除所有样本中具有相同值的特征

---
!!! tip "步骤"  
    1. 在头部引入头文件：`from sklearn.feature_selection import VarianceThreshold`  
    2. 下面的代码
        
        ```Python
        def variance_demo():  
            """  
            过滤低方差特征  
            :return:    """    #     获取数据  
            data=pd.read_csv("va.csv")  
            data=data.iloc[:,3:-1]  
            print("data:\n",data)  
            # 实例化一个转换器类  
            transfer=VarianceThreshold(threshold=1000)  
            # 调用fit_transform  
            data_new=transfer.fit_transform(data)  
            print("data_new:\n",data_new,data_new.shape)  
            return None
        ```
    1. 最后的输出结果为：（要注意调整输出特征的条件）
        ![](png/Pasted%20image%2020250826231618.png)

#### 相关系数的求法  
- 皮尔逊相关系数
    - 反应变量之间相关关系密切程度的统计指标
- 公式

    $$
    r=\frac{n\sum xy-\sum x\sum y}{\sqrt{n\sum x^2-(\sum x)^2}\sqrt{n\sum y^2-(\sum y)^2}}
    $$

    值的绝对值越接近 1，则两个变量的相关程度越大

!!! tip "方法"  
    - 在前面引入：`from scipy.stats import pearsonr`    
    - 加上下面的代码：
        
    ```Python
    # 计算两个变量之间的相关系数  
    r1=pearsonr(data["quantity"],data["unit_price"])  
    print("r:\n",r1)  
    r2=pearsonr(data["total_sales"],data["unit_price"])  
    print("r:\n", r2)
    ```

计算之后的相关性很高的特征怎么办  

1. 选择其中的一个
2. 加权求和
3. 主成分分析  
从而将相关性高的成分处理掉  

### 主成分分析
#### 什么是主成分分析  
- 定义：将高维数据转化为低位数据的过程中，可能会有舍弃原有的数据，创造新的变量的过程
- 作用：是数据维数压缩，尽可能降低原数据的维数（复杂度），损失少量信息
- 应用：回归分析或者聚类分析当中

我们在拍出三维的视图时，在将三维降到二维的过程中，怎么弄使得信息的损失最少（降维后的信息保留原有的特征）  

将二维的数据转化为一维的时候，找到一个合适的直线，通过矩阵运算得出主成分分析的结果  

#### API  
怎么在 sklearn 中使用  

- `sklearn.decomposition.PCA(n_components=None)`
    - 将数据分解为较低维数空间
    - n_components:
        - 小数：表示保留百分之多少的信息
        - 整数：减少到多少的特征
    - PCA. fit_transform (X)
    - 返回值为：转换后指定维度的 array

#### 计算  
!!! tip "代码"  
    -     开头调用：`from sklearn.decomposition import PCA`  
    - 之后的代码：
        
    ```Python
    def pca_demo():  
        """  
        pca降维  
        :return:    """    data=[[2,8,4,5],[6,3,0,8],[5,4,9,1]]  
      
        #实例化一个转化器  
        transfer=PCA(n_components=0.95)  #保留了95%的信息
      
        #调用  
        data_new=transfer.fit_transform(data)  
        print("data_new:\n",data_new)  
        return None
    ```
最后的输出结果：  

> [!tip] 结果  
> data_new:  
> [[-1.28620952e-15  3.82970843e+00]  
> [-5.74456265e+00 -1.91485422e+00]    
> [ 5.74456265e+00 -1.91485422e+00]]

#### 案例分析：探究用户对物品类别喜好的细分  

用户 物品类别之间的关系  

1. 需要将 user_id 和 aisle 放在同一个表中  
2. 将行索引变为用户 ID，列索引变为某一物品的数量（检查表和透视表）
3. 特征冗余过多——>PCA 降维

!!! tip "合并"  
    1. 合并 aisles 和 products, 使用 `pd.merge`  
    2. `aisles` 和 `products` 是两个要合并的 DataFrame（数据表）  
    3.  `on=["aisle_id","aisle_id"]` 指定了合并的依据是这两个 DataFrame 中都存在的 `aisle_id` 列  
    简单说，这段代码的作用是：根据 `aisle_id` 这一列，将 `aisles` 和 `products` 两个数据表中相关联的记录合并到一起，生成一个新的数据表 `tab1`。

!!! tip "所有的步骤“  
    1. 首先应该安装 jupyter，在 cmd 中使用 pip 安装  
    2. 之后在 cmd 中打开，选择打开的目录为数据所在的文件 `jupyter notebook --notebook-dir=D:\dev\pythonProject1`  
    3. 引入头文件，读取数据：  
    ```Python  
    !pip install pandas  
    import pandas as pd  
    ```  
    4. 进行数据的读取：  
    ```Python  
    products=pd.read_csv("./archive/products.csv")  
    order_products=pd.read_csv("./archive/order_products__prior.csv")  
    orders=pd.read_csv("./archive/orders.csv")  
    aisles=pd.read_csv("./archive/aisles.csv")  
    ```  
    5. 进行多次的数据的合并  
        `tab1 = pd.merge(aisles,products, on=["aisle_id","aisle_id"])`  
        `tab2 = pd.merge(tab1 , order_products, on="product_id")`  
        `tab3=pd.merge(tab2,orders,on="order_id")`  
    6. 找到所需数据之间的关系（# 3.找到user_id和aisle之间的关系）  
        `table = pd.crosstab(tab3["user_id"],tab3["aisle"])`  
    7. 之后进行数据的降维（之前的内容）（为了简化，可以取一部分）  
        `data=table[:20000]`

`206209 rows × 134 columns` 这是原始的数据，经过简化之后的数据为  
![](png/Pasted%20image%2020250827220950.png)  
可见，在 95%的简化之后，列数据减少了很多  

### 总结  
![](png/机器学习.png)  

## 分类算法  
目标值：**类别**的话就是分类算法  

### sklearn 的转换器和预估器  
#### 转换器  
我们之前一直在用的就是转换器（特征工程的父类）  

1. 实例化（就是一个转换器类）
2. 调用 fit_transform（先是 fit，之后是 transform）
3. 在标准化的时候
    1. fit：计算每一列的平均值和标准差
    2. transform：带入公式中进行最终的转换

#### 估计器  
估计器 (estimator)，是一类实现了算法的 API  

!!! tip "步骤"  
    1. 实例化一个 estimator  
    2. estimator. fit (x_train, y_train), 这里的 fit 也是在计算  
        1. 调用完毕之后模型已经生成了  
    3. 模型的评估：  
        1. 直接比对真实值和预测值  
            y_predict=estimator. predict (x_test)  
            y_test\==y_predict  
        2. 计算准确率  
            estimator. score (x_test, y_test)

### K-近邻算法（KNN 算法）  
#### 什么是  
通过你的邻居判断出你的类别  

- 原理：  
    一个样本中在特征空间中的 k 个**最相似**（即特征空间中**最临近**）的样本中的大多数属于某一个类别，则该样本也属于这个类别  
    但是 k=1 的时候容易收到异常值的影响
- 计算距离：  
    距离公式，我们在几何中常用的欧式距离

    $$
    \begin{aligned}&\text{a(a1,a2,a3),b(b1,b2,b3)}\\&\sqrt{(a1-b1)^{2}+(a2-b2)^{2}+(a3-b3)^{2}}\end{aligned}
    $$

    还有曼哈顿距离：就是绝对值距离  
    明可夫斯基距离
- 电影类型分析  
    k=1：爱情片（最近的已知电影）  
    k-2：爱情片  
    ……  
    k=6 时，不行，因为已知的一共只有六个  
    当我们的 k 值取值过大时，容易分错（样本不均衡的时候，容易收到影响） 
- 同样，像之前一样，因为要有求距离，所以需要无量纲化处理
    - 使用标准化无量纲化处理

---

#### k-近邻算法 API    
-  `sklearn.neighbors.KNeighborsClassifier(n_neighbors=5,algorithm='auto')`
    -  `n_neighbors`: `int`, 可选（默认= 5），`k_neighbors` 查询默认使用的邻居数 （这就是 k 值）
    -  `algorithm`: `{'auto', 'ball_tree', 'kd_tree', 'brute'}`，可选用予计算最近邻居的算法：`'ball_tree'` 将会使用 `BallTree`，`'kd_tree'` 将会使用 `KDTree`。`'auto'` 将尝试根据传递给 `fit` 方法的值来决定最合适的算法。（不同实现方式影响效率）

#### 案例：鸢尾花案例  
!!! tip "步骤"  
    1. 获取数据  
    2. 数据集划分  
    3. 特征工程  
        1. 标准化  
    4. KNN 预估器流程  
    5. 模型评估

代码：
```Python
from sklearn.datasets import load_iris  # 引入数据
from sklearn.model_selection import  train_test_split  # 模型划分
from sklearn.neighbors import KNeighborsClassifier  # KNN算法
from sklearn.preprocessing import StandardScaler  # 标准化
  
  
def knn_iris():  
    """  
    使用算法对鸢尾花进行分类  
    :return:    """    # 获取数据  
    iris=load_iris()  
    # 划分数据集  
    x_train,x_test,y_train,y_test=train_test_split(iris.data,iris.target,random_state=22)  
  
    # 标准化  
    transfer=StandardScaler()  
    x_train=transfer.fit_transform(x_train)# 对训练集进行标准化  
    x_test=transfer.transform(x_test)  
    # 算法预估器  
    estimator=KNeighborsClassifier(n_neighbors=3)  
    estimator.fit(x_train,y_train)  
    # 模型评估  
        # 方法一：比较真实值和预测值  
    y_predict=estimator.predict(x_test)  
    print("y_predict:\n",y_predict)  
    print("直接比较的准确率：\n",y_test==y_predict)  
        # 方法二：计算准确率  
    score=estimator.score(x_test,y_test)  
    print("准确率为\n",score)  
    return None  
# 代码1：KNN算法鸢尾花分类  
knn_iris()
```

#### 总结  
- 优点：
    - 简单，易于理解，易于实现，无需训练
- 缺点：
    - 懒惰算法：对测试样本分类是的计算量大，内存开销大
    - 必须指定 k 值，k 值选择不当则分类精度不能保证
- 使用场景：小数据场景

### 模型选择与调优
#### 什么是交叉验证  
**交叉验证**：将得到的**训练数据**分为训练和验证集： 将数据氛围 4 分。其中的一份作为验证集，然后进行 4 次测试，每次都换不同的验证集，即得到 4 组模型的结果，取平均值作为最终的结果  

针对**训练集**进行这样的划分，从而使得训练得出的模型更加准确  

#### 超参数网格搜索  
有很多参数是需要手动指定的（k），手动过程过于复杂，所以需要对模型预设几种超参数组合。**每组超参数使用交叉验证，得到最有效的参数**  

#### 模型选择与调优 API  
- `sklearn.model_selection.GridSearchCV(estimator, param_grid=None, cv=None)`
    - 对估计器的指定参数值进行详尽搜索
    - `estimator`：估计器对象 （也是一个预估器的类）
    - `param_grid`：估计器参数（`dict`，如 `{"n_neighbors": [1, 3, 5]}` ）（k 的取值，使用字典来传进来）
    - `cv`：指定几折交叉验证 
    - `fit()`：输入训练数据 
    - `score()`：准确率 
    - 结果分析: 
        - 最佳参数：`best_params_` 
        - 最佳结果：`best_score_` 
        - 最佳估计器：`best_estimator_` 
        - 交叉验证结果：`cv_results_`

与上一节相比更改的地方：  

1. 导入新的库
    ```Python
    from sklearn.model_selection import GridSearchCV
    ```
2. 算法预估器变为下面的形式：
    ```Python
    # 算法预估器  
    estimator=KNeighborsClassifier()  
    # 加入网格搜索和交叉验证  
    # 参数准备  
    param_dict={"n_neighbors":[1,3,5,7,9,11]}  
    estimator=GridSearchCV(estimator,param_grid=param_dict,cv=10)  
    estimator.fit(x_train,y_train)
    ```
3. 最后的输出结果更加丰富
    ```Python
    # 最佳参数：`best_params_`  
    print("最佳参数:\n",estimator.best_params_)  
    # 最佳结果：`best_score_`  
    print("最佳结果:\n", estimator.best_score_)  
    # 最佳估计器：`best_estimator_`  
    print("最佳估计器:\n", estimator.best_estimator_)  
    # 交叉验证结果：`cv_results_`  
    print("最佳交叉验证结果:\n", estimator.cv_results_)
    ```

输出结果：  
![](png/Pasted%20image%2020250828213621.png)  

我们现在的流程已经全部有了  

#### 预测 facebook 签到位置  
kaggle 上的比赛  

- train. csv. test. csv
    - x, y 坐标
    - 准确性：定位准确性
    - 时间：时间戳
    - place_id：业务的 ID，预测的目标  
还是使用 `D:\dev\pythonProject1>jupyter notebook`  新建（右侧）python 3  
这个 jupyter 好像是一步步手动运行的

!!! tip "步骤"
1. 数据的处理：（特征值 x 和目标值 y）
    1. 缩小数据范围的处理（将 x 的坐标放在 2,2.5，y 放在 1,1.5 之间）`data=data.query("x<2.5&x>2&y>1&y<1.5")  data`
    2. （b.time->年月日和时分秒），使得时间有意义化
    3. 过滤签到次数少的地点
2. 特征工程：标准化
3. KNN 算法预估流程
4. 模型选择与调优
5. 模型评估

注意这个课程是需要在看完数据分析与挖掘基础之后再来学习的，数据分析课程下边会有链接点击后直接跳转至此课程的  

---
发现最难的地方还是在数据处理上  

### 朴素贝叶斯算法  
#### 什么是朴素贝叶斯算法的分类方式  
KNN 直接出分到哪一类了  
但是朴素贝叶斯算法分完之后是各种结果的概率值  
![](png/Pasted%20image%2020250829132703.png)  

#### 概率的基础知识  
- 概率的定义：
    - 一件事情发生的可能性
        - 扔出一个硬币，结果头像朝上的可能性
    - P (X)：取值在[0,1]

相互独立的定义，还有贝叶斯公式  

$$
P(C|W)=\frac{P(W|C)P(C)}{P(W)}
$$

- 朴素贝叶斯算法：  
    朴素+贝叶斯  
    朴素就是假定特征之间是相互独立的  
    贝叶斯就是贝叶斯公式
- 应用场景：  
    文本分类（以单词作为特征，词与词之间是相互独立的）

$$
P(C|F1,F2,...)=\frac{P(F1,F2,...|C)P(C)}{P(F1,F2,...)}
$$

也就是这里的 $F_1,F_2$ …… 这些是相互独立的，所以右边分子上的条件概率可以分开计算（乘出来）  

$$
变为P(F_1|C)P(F_2|C)……
$$

但是当这里的**乘的一项为 0 时怎么办**（样本的数据太少）：    

- 拉普拉斯平滑系数：

    $$
        P(F1|C)=\frac{Ni+\alpha}{N+\alpha m}
    $$

    $\alpha$ 为指定的系数，一般为 1，m 为训练集（不仅仅是 C 集）中统计出的特征词的个数（特征词的种类），$Ni$ 为该特征 **F₁** 在类别 **C** 中出现的次数  
    **N** 通常表示在类别 **C** 中所有特征（或词语）出现的总次数，也就是类别 **C** 下的样本总量（或词频总和）  
    **必须是在 C 集**的条件下

#### API  
- sklearn.naive_bayes.MultinomialNB(alpha = 1.0)
  -  朴素贝叶斯分类
  -  alpha: 拉普拉斯**平滑系数**

#### 案例：20 类新闻分类  
!!! tip "步骤"    
    1. 获取数据`from sklearn.datasets import fetch_20newsgroups`  
    2. 划分数据集（文字的）`from sklearn.feature_extraction.text import TfidfVectorizer`  
    3. 特征工程  
        文本特征抽取  
    4. 朴素贝叶斯预估器流程`from sklearn.naive_bayes import MultinomialNB`  
    5. 模型评估

代码：  
```Python
def nb_news():  
    """  
    朴素贝叶斯算法新闻分类  
    :return:    """    # 1获取数据  
    news=fetch_20newsgroups(subset="all")  
    #2. 划分数据集  
    x_train,x_test,y_train,y_test=train_test_split(news.data,news.target)  
    #3. 文本特征抽取  
    transfer=TfidfVectorizer()  
    x_train=transfer.fit_transform(x_train)  
    x_test=transfer.transform(x_test)  
    # 朴素贝叶斯算法预估器  
    estimator=MultinomialNB()  
    estimator.fit(x_train,y_train)  
    # 模型评估  
    # 方法一：比较真实值和预测值  
    y_predict=estimator.predict(x_test)  
    print("y_predict:\n",y_predict)  
    print("直接比较的准确率：\n",y_test==y_predict)  
    # 方法二：计算准确率  
    score=estimator.score(x_test,y_test)  
    print("准确率为\n",score)  
    return None
```

#### 总结  
- 优点：
    - 有稳定的分类效率
    - 对缺失的数据不敏感
    - 分类准确度高，速度快
- 缺点：
    - 由于使用了样本属性独立性的假设，所以如果特征属性有关联时其效果不好

### 决策树  
#### 认识决策树  
决策树，就是 if-else 的方式进行分类  
![](png/Pasted%20image%2020250829154631.png)  

#### 决策树的分类原理详解  
怎么知道原理的先后  

已知四个特征，预测是否贷款给某个人  

- 有房子的都是
    - 在没有房子的里面：有工作的都是  
        已经能决定是否贷款了
- 先看年龄，再看信贷情况，最后看工作看了三个特征
- 所以可以看出最高效的特征为第一种

!!! tip "原理"    
    - 信息熵：信息增益  
    - 信息熵的定义：  
        - H 的专业术语称之为信息熵，单位为**比特**
    
            $$
                    H(X)=-\sum_{i=1}^nP(x_i)logbP(x_i))
            $$

---
1. 信息：消除随机不确定性的东西  
    小明年龄：今年 18 岁（是信息）  
    小华说小明明年 19 岁（在上面的基础上的推论，不是信息）
2. 如何量化信息量：  
    信息熵：$H(X)=-\sum_{i=1}^nP(x_i)logbP(x_i))$，这里对数的底数一般是 2

已知某人的年龄，工作，房子，信贷情况等等，消除是否贷款的不确定性  
怎么使用上面的信息熵的公式求出 H (x)？  

$$
\bar{H}(\text{总})=-(6/15^{\star}\log6/15+9/15^{\star}\log9/15)
$$

3. 信息增益：

    $$
        g\left(D,A\right)=H\left(D\right)-H\left(D\right|A)
    $$

    计算公式    

    $$
    H(D|A)=\sum_{i=1}^n\frac{|D_i|}{|D|}H(D_i)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{k=1}^K\frac{|D_{ik}|}{|D_i|}\log\frac{|D_{ik}|}{|D_i|}
    $$

    使用总的信息熵减去知道某个信息之后的信息熵就是信息增益  
    例如上面的例子中，知道年龄之后的信息增益为为  
    A 是用于划分数据集的特征，如年龄，性别等，将数据集 D 划分为多个子集  
    特征 A 取第 i 个值时，对应的样本子集（如 A 是 “性别”，D₁= 男性样本，D₂= 女性样本）

#### 决策树的 API  
- class sklearn.tree.DecisionTreeClassifier(criterion='gini', max_depth=None,random_state=None)
  * 决策树分类器
  * criterion:默认是'gini'系数，也可以选择信息增益的熵'entropy'
  * max_depth:树的深度大小（深度过大的话会过拟合）
  * random_state:随机种子

过拟合会导致模型泛化能力差，即过度适合当前样本集而缺乏适应（预测）新样本的能力  

完整的代码：  
```Python
def decision_iris():  
    """  
    用决策树对鸢尾花进行分类  
    :return:    """    # 1. 获取数据集  
    iris=load_iris()  
    # 2. 划分数据集  
    x_train,x_test,y_train,y_test=train_test_split(iris.data,iris.target,random_state=22)  
    # 3. 决策树预估器  
    estimator=DecisionTreeClassifier(criterion="entropy")  
    estimator.fit(x_train,y_train)  
    # 4，模型评估  
        # 方法一：比较真实值和预测值  
    y_predict=estimator.predict(x_test)  
    print("y_predict:\n",y_predict)  
    print("直接比较的准确率：\n",y_test==y_predict)  
        # 方法二：计算准确率  
    score=estimator.score(x_test,y_test)  
    print("准确率为\n",score)
```

新加的头文件：`from sklearn.tree import DecisionTreeClassifier`  

#### 决策树的可视化  
保存树的结构到 dot 文件  

* sklearn.tree.export_graphviz() 该函数能够导出DOT格式
* tree.export_graphviz(estimator,out_file="tree.dot",feature_names=[""])  
estimator：所求的预估器  
out_file="tree.dot"：路径  

---
之后需要借助网站显示文本文件  

!!! tip "步骤"  
    - `from sklearn.tree import DecisionTreeClassifier,export_graphviz`  
    ```Python  
    # 可视化决策树  
    export_graphviz(estimator, out_file="iris_tree.dot")  
    ```  
但是没有分类的特征名字：  
`export_graphviz(estimator, out_file="iris_tree.dot",feature_names=iris.feature_names)`  
这样就有特征的名字了  

查看的网址：  
https://dreampuf.github.io/GraphvizOnline/  

最后的树：  
![](png/graphviz.png)  

#### 总结  
- 优点：
    - 简单，易于理解
    - 可视化-可解释能力强（深度学习神经网络相反）
- 缺点
    - 处理过于复杂的网络时，容易发生过拟合
- 改进：
    - 剪枝 cart 算法
    - 随机森林

#### 泰坦尼克号乘客生存的预测案例  
1. 获取数据
2. 数据处理
    1. 缺失值处理
    2. 有多个特征是类别，将特征值转换为字典类型的
    3. 筛选特征值和目标值
    4. 划分数据集
    5. 特征工程：字典特征获取
    6. 决策树预估器流程
    7. 模型评估