## 人工智能概述  
### 可计算思想
费马猜想与费马定理  
地方太小，写不下（费马大定理）  
英国的数学家怀尔斯证明了这个定理是成立的  

---
如何判断一个**问题**是否可以计算  
证明算术公理的相容性  

- 完备性
- 一致性
- 可判定性

图灵机的模型：程序  
任何可以计算的函数都可以使用**图灵机计算**  

|         计算载体         |    提出学者    |  计算角度   |
| :------------------: | :--------: | :-----: |
|        原始递归函数        | 哥德尔（Godel） |  数学的形式  |
| λ - 演算（λ - calculus） | 丘奇（Church） | 数理逻辑的形式 |
|         图灵机          | 图灵（Turing） |  机械的形式  |  

!!! tip "图灵测试"  
    放入机器和人  
    出 20 个问题分别进行回答  
    收集到机器和人回答的结果  
    请法官进行分辨  
    如果能区分的话，机器不具备人的认知水平  
    反之通过了图灵测试，具有**人类的智能水平**    

**习题**：  
![](png/Pasted%20image%2020250921201132.png)  

!!! tip "解题"  
    1. 首先对数字进行编码（12 就是 12 个 1）  
    2. 初始化纸袋与状态  
        1. 初始状态为 $q_0$  
    3. 当前的符号为 1 的话，将其擦除，转移到状态 $q_1$，指针右移  
        1. 重复操作，擦除所有的 12 的 1  
        2. 指针指向+时, 擦除，转向状态 $q_2$，右移  
    4. 在“$q_2$”状态下，为 1 时，保持 1 不变，转移到状态 $q_3$  
        1. 右移，处理完成所有的 8 的 1  
    5. 此后指针开始左移，将原来的 12 位置的空白转变为 1，从而实现加法的效果

### 智能计算方法  
- **领域**人工智能：
    - 照葫芦画瓢、任务导向（alphago）
- **通用**人工智能
    - 举一反三、从经验中学习
- **混合增强**人工智能（多种智能体的混合形式）（人类是**智能的总开关**）
    - 外骨骼机器人
    - 人类智能＋机器智能（达芬奇外科手术机器人）
    - 人、机、物联网

---
**主流**的方法：  

- **符号主义**人工智能为核心的逻辑推理
    - IBM“沃森”的推理
- **数据**驱动的机器学习：
    - 采集海量的人脸的数据，基于学习到的数据完成人脸分辨的任务（像素点的分布）
- **探索与利用**为核心的**强化**学习
    - 用问题牵引——>从经验中学习

|   学习模式    |         优势         |         不足          |
| :-------: | :----------------: | :-----------------: |
| 用**规则**教  | 与人类**逻辑推理**相似，解释性强 |    难以构建完备的知识规则库     |
|   用数据学    |      直接从数据中学       | 以深度学习为例：依赖于数据、解释性不强 |
| 用**问题**引导 |   从经验中进行能力的持续学习    |    非穷举式搜索而需更好策略     |

### 人工智能的历史进展  
人工智能发展的三次低谷  
人工智能的目的，是使机器能够具有（**模拟、延伸和扩展人类智能的能力**，具体可细分为感知、理解、推理、学习、决策等与人类智能相关的核心功能）。  

## 逻辑与推理  
### 命题逻辑  
- 定义：命题是一个**能确定**是真或者是假的陈述句
    - 原子命题：最简单的基本的命题
    - 复合命题：由原子命题组成的命题

---
通过命题连接词：  

|   p   |   q   | $\neg p$ | $p \land q$ | $p \lor q$ | $p \to q$ | $p \leftrightarrow q$ |
| :---: | :---: | :--------: | :-----------: | :----------: | :---------: | :---------------------: |
| False | False |    True    |     False     |    False     |    True     |          True           |
| False | True  |    True    |     False     |     True     |    True     |          False          |
| True  | False |   False    |     False     |     True     |    False    |          False          |
| True  | True  |   False    |     True      |     True     |    True     |          True           |

- **条件命题前者为假时，无论后者是真假，命题一定为真**
    - 如果 p，那么 q 是一种蕴含的关系（充分条件），也就是 p 是 q 的子集
    - p 不成立说明其是一个空集，是任何集合的子集，所以一定为真

---
???+ tip "**推理规则**"  
    - 假言推理：  
    - 与消解：  
    - 与导入：  
    - 双重否定：推导出本身是成立的  
    - 单向消解或单项归结：  
    - 消解或归结：

例题：  
![](png/Pasted%20image%2020250927205114.png)  
V 是或，反过来是和，——>为条件推理  

---

#### 范式  
- 重要的概念，将命题公式化归为一种标准的形式，可以进行两个命题的等价判定
    - 析去范式：$\text{假设}\alpha_\mathrm{i(i=1,2,\cdots,k)}\text{为简单的合取式,则}\alpha=\alpha_1\lor\alpha_2\lor\cdots\lor\alpha_\text{k为析取范式}$
    - 合取范式：$假设\alpha_i(i=1,2,\cdots,k)为简单的析取式,则\alpha=\alpha_1\wedge\alpha_2\wedge\cdots\wedge\alpha_k为合取范式$
    - 两者合称范式
    - **析取范式由合取范式组成**，合取范式由析取范式组成
- 析取范式**不成立**当且仅当每个简单的合取**都不成立**
- 合取范式是成立的，当且仅当每个简单的析取式是**成立**的
- **任一命题**的公式都有**等**值的析取范式和合取范式（不唯一的）

### 谓词逻辑  
命题逻辑——>谓词逻辑  

**谓词逻辑中**：个体、谓词和量词  

- 个体：可以**独立存在**的具体或者抽象的概念（x 的**变元**）
- 谓词：刻画**个体属性或者是描述个体之间关系存在性**的元素，其**值为真或者是假**
    - 与**函数**的不同
        - 函数使用之后为因变量
        - 谓词使用之后变成了命题
- 事实符号化：
- 量词的引入：
    - 全称量词：所有的
    - 存在量词：存在
    - 两者统称为量词

=== "全称量词"  
    - 谓词 $P(x)$：x 能够**制造**工具。$\forall x P(x)$ 表示定义域中的所有个体能够制造工具。$P(\text{小王})$ 表示小王能够制造工具。  
=== "存在量词"  
    - 谓词 $P(x)$：x 能够制造工具。$\exists x P(x)$ 表示定义域中的存在某个 / 某些个体能够制造工具。$P(\text{小王})$ 表示小王能够制造工具（该命题或者为真、或者为假）。  

- 约束**变元**：有量词的约束 (x)
- 自由变元：没有量词的约束
- 自由变元既可以存在于量词的约束范围之内，也可以存在量词的约束范围之外

下面的公式是成立的：  

$$
\begin{gathered}(\forall x)(A(x)\lor B)\equiv(\forall x)A(x)\lor B\$\forall x)(A(x)\wedge B)\equiv(\forall x)A(x)\wedge B\$\exists x)(A(x)\lor B)\equiv(\exists x)A(x)\lor B\$\exists x)(A(x)\wedge B)\equiv(\exists x)A(x)\wedge B\end{gathered}
$$

在约束变元相同的情况下，量词的运算满足分配率  

$$
\begin{gathered}(\forall x)(A(x)\lor B(x))\equiv(\forall x)A(x)\lor(\forall x)B(x)\$\forall x)(A(x)\wedge B(x))\equiv(\forall x)A(x)\wedge(\forall x)B(x)\$\exists x)(A(x)\lor B(x))\equiv(\exists x)A(x)\lor(\exists x)B(x)\$\exists x)(A(x)\wedge B(x))\equiv(\exists x)A(x)\wedge(\exists x)B(x)\end{gathered}
$$

???+ example "描述以下的事实"  
    - 所有的国王都头戴皇冠。  
    - “所有的国王都头戴皇冠” 表示的含义为 “对于**所有的x**，如果x是国王，那么x头戴皇冠”，符号化表示为$(\forall x)(King(x) \to Head\_On(Crown, x))$。其中x是变量符号，由于x受到全称量词的约束，因此x是约束变元；Crown是一个常量符号，表示皇冠；$King(x)$是一个一元谓词，表示x是国王，$Head\_On(Crown, x)$是一个二元谓词，表示x头戴皇冠。

- 公式中存在多个量词时，多个量词都是同类型的，那么量词的顺序是可以互换的
- 反之是不可以互换的

$$
\begin{aligned}&(\forall x)(\forall y)A(x,y)\Leftrightarrow(\forall y)(\forall x)A(x,y)&&(\exists y)(\forall x)A(x,y)\Leftrightarrow(\forall x)(\exists y)A(x,y)\\&(\exists x)(\exists y)A(x,y)\Leftrightarrow(\exists y)(\exists x)A(x,y)&&(\exists x)(\forall y)A(x,y)\Leftrightarrow(\forall y)(\exists x)A(x,y)\\&(\forall x)(\forall y)A(x,y)\Rightarrow(\exists y)(\forall x)A(x,y)&&(\forall x)(\exists y)A(x,y)\Rightarrow(\exists y)(\exists x)A(x,y)\\&(\forall x)(\forall y)A(x,y)\Rightarrow(\exists x)(\forall y)A(x,y)&&(\forall y)(\exists x)A(x,y)\Rightarrow(\exists x)(\exists y)A(x,y)\end{aligned}
$$

#### 项与原子谓词公式  
- 项是描述对象的逻辑表达式
    - 常量符号和变量符号是项
    - 项组成的函数是项：$\mathrm{f(t1,t2,\cdots,tn)}$
    - 有限次使用上述的规则产生的符号串是项
- **原子谓词公式**：项数目等于对象的命题吗
- **合式**公式：
    - 命题常项、命题变项、原子谓词公式为合式公式
    - A 为合式公式，则非 A 也是**合式公式**
    - A、B 是合式公式，则 $A\land B、A\lor B、A\to B、B\to A、A\leftrightarrow B$ 是合式公式
    - A 为合式公式，x 为个体变项、$(\exists x)A(x)\neq0(\forall x)A(x)$ 也是合式公式
    - 有限次使用上述的法则构成的表达式组成的是合式公式

---
???+ example "使用合式公式描述"  
    ![](png/Pasted%20image%2020250927212805.png)  
    
    ---
    - 解 (1) $Like(\text{Tom}, \text{Football}) \land Like(\text{Tom}, \text{Basketball})$。其中，$Like(\text{Tom}, x)$表示 Tom 喜欢x，Football 和 Basketball 分别表示踢足球和打篮球。
    - (2) $(\forall x)(Classmate(\text{Tom}, x) \to Like(x, \text{Tom}))$。其中，$Classmate(\text{Tom}, x)$表示x是 Tom 的同学，$Like(x, \text{Tom})$表示x喜欢 Tom。
    - (3) $\neg (\forall x)(Boy(x) \to Like(x, \text{Basketball}))$。其中，$Boy(x)$表示x是男生，$Like(x, \text{Basketball})$表示x喜欢打篮球。

---

#### 推理规则  
- 全称量词消去：
- 全称量词引入：
- 存在量词消去：
- 存在量词引入：

???+ example "证明"  
    ![](png/Pasted%20image%2020250927213342.png)

#### 自然语言的形式化  
![](png/Pasted%20image%2020250927213719.png)  

#### 专家系统的构成  

```mermaid
flowchart LR
    A["问题逻辑化<br/>(机器可理解)"] --> B["推理机<br/>(专家系统)"]
    C["对某个领域的自然语言、<br/>文本语句等信息进行逻辑化<br/>(machine readable knowledge)"] --> B
    B --> D["答案输出"]
```
课后题：  
![](png/Pasted%20image%2020250927213940.png)  

### 知识图谱推理  
#### 基本概念  
- 是包含多种关系的图
- 每个节点是一个实体，任意两个节点之间的边表示两个节点间存在的关系
- 实体中存在的关系进行推理，可以得到**新的知识**

#### 归纳
归纳逻辑程序设计算法：使用一阶谓词逻辑进行知识表示  
将目标谓词作为推理的规则  

- 目标谓词：
- 目标谓词只有**一个正例**
- 反例：不会显式给出，而是从知识图谱中构建出来
- 背景知识：

推理的思路：逐步给目标的谓词添加**前提约束谓词**，使得经过目标谓词之后**不覆盖任何的反例**  
通过**信息增益**的方式来评判前提约束谓词的好坏  
**FOIL 算法**  
![](png/Pasted%20image%2020251007221123.png)  
**信息增益**  
???+ tip "规则"  
    从实例（正例、反例背景知识中）出发，不断测试所得的推理规则**是否包含反例**、一旦不包含反例，则学习结束，充分展示了“归纳学习”的能力。则学习之后，将规则幅值  

#### 习题  
![](png/Pasted%20image%2020251007221423.png)  
???+ info "解答"  
    - FOIL（First-Order Inductive Learner）算法通过寻找**覆盖正例且不覆盖反例**的规则  
    - 在家庭关系中，“Mother” 的定义可结合 “Couple” 和 “Father” 推导：  
    - 若 **X 和 Y 是 Couple**，且 **Y 是 Z 的 Father**，则 **X 是 Z 的 Mother**。  
    - 带入规则  
        - $X = \text{James}$，$Y = \text{David}$，$Z = \text{Ann}$；  
        - 已知 “James 和 David 是 Couple”，且 “David 是 Ann 的 Father”；  
        - 因此，满足规则条件，可推出 “James 是 Ann 的 Mother”。  
    - 验证关系的一致性  
        David 也是 Mike 的 Father（符合 “Sibling” 的家庭逻辑）；同时 James 作为 David 的 Couple，自然也成为 Mike 的 Mother，这与整体家庭关系的一致性**相互印证**，说明推导规则有效。  

### 因果推理  
#### 因果推理基本概念  
**先有鸡还是先有蛋**的问题  
辛普森悖论  

##### 主要模型  
- 结构因果模型
- 因果图：一个无回路的有向图
    - DAG 称为贝叶斯网络  
???+ tip "结构因果模型"  
    - 有两组变量集合 U 和 V 以及一组函数 f 组成（就是**因变量**的意思）
    - X 变量出现在 Y 幅值的函数中，则 X 是 Y 的**直接原因**。X 是 Y 的直接原因或者是其他原因，均称 X 是 Y 的**原因**
    - U 中的成为外生变量，V 中的为内生变量（外生是**原始**的变量）（内生都是外生的子代）
    - 每一个结构因果模型都有一个因果图与其对应

???+ tip "因果图模型"  
    - 原因：在因果图中 Y 是 X 的孩子，则 X 是 Y 的直接原因；Y 是 X 的后代，则 X 是 Y 的潜在原因  
    - 可以有效表示**联合概率**分布

---
联合概率密度  

$$
P(x_1,x_2,\cdots,x_d)=\prod_{j=1}^dP(x_j|x_{pa(j)})
$$

这就是 d 个变量的联合概率密度  

$$
x_{pa(j)}\text{表示节点}x_j\text{的父节点集合(所有指向}x_j\text{的节点})
$$

???+ tip "作业：写出联合概率形式"  
    ![](png/Pasted%20image%2020251007223631.png)  

    $$
    P(X2​,X3​,X4​,X5​,X1​,X6​,Xi​,Xj​)=P(X2​)P(X3​)P(X4​∣X2​)P(X5​∣X3​)P(X1​∣X2​,X3​)P(X6​∣X1​,Xi​)P(Xj​∣X1​,X5​)​
    $$

#### 因果图结构  
##### 链结构  
- 是一种基本结构
- 包含三个节点两条边，其中一条由第一个节点指向第二个节点，另外一条由第二个节点指向第三个节点
- ![](png/Pasted%20image%2020251007224127.png)
- X，Y 在给定 Z 时条件独立
- 所以有**链中的条件独立性定理**：  
    若 X 和 Y 之间只有一条单向的路径，变量 Z 是**截断** (intercept) 该路径的集合中的任一变量，则在**给定 Z 时，X 和 Y 条件独立**。

##### 分连  
- 三个节点，两条边
- 两条边分别由第一个点指向第二三个
- ![](png/Pasted%20image%2020251007224606.png)

    $$
    \begin{aligned}&P(X,Y|Z)=\frac{P(X,Y,Z)}{P(Z)}=\frac{P(Z)P(X|Z)P(Y|Z)}{P(Z)}\\&=P(X|Z)P(Y|Z)\end{aligned}
    $$

- 定理：Z 是 X, Y 的共同原因，且 X->Y 只有一条路径，则在给定 Z 的条件, XY 条件独立

##### 汇连图
- ![](png/Pasted%20image%2020251007224807.png)
- 此时的 X, Y 是**相关的**，没有上面的条件独立的性质
- 定理：上面，X, Y 是相互独立的，但是在给定 Z 和 Z 的后代时，X, Y 是**相关的**

##### D 分离  
- 定义 2.18 D - 分离：路径 p 被限定集 Z 阻塞 (block) **当且仅当**：
    - (1) 路径 p 含有**链结构** A→B→C 或分连结构 A←B→C 且中间节点 B 在 Z 中，或
    - (2) 路径 p 含有汇连结构 A→B←C 且汇连节点 B 及其后代都不在 Z 中。  
若 Z 阻塞了节点 X 和节点 Y 之间的每一条路径，则称给定 Z 时，X 和 Y 是 D - 分离，即给定 Z 时，X 和 Y **条件独立**。（**分离则独立**）  

???+ tip "题目"  
    1. 若限定集为∅时，X 和 T 相互独立。因为 X 和 T 之间含有一个汇连结构 X→Z←Y，且 Z 及其后代都不在限定集∅中，此时 X 和 T 是有向分离，即 X 和 T 相互独立；  
    2. 若限定集为 {W} 时，X 和 T 是相关的。因为 X 和 T 之间含有一个链式结构 Y→S→T（S 不在限定集中），一个分连结构 Z←Y→S（Y 不在限定集 {W} 中），一个汇连结构 X→Z←Y（Z 的后代 W 在限定集 {W} 中），根据 D - 分离的定义，这些结构都**无法阻塞** X 和 T 之间的路径，因此 X 和 T 是有向连接的，即 X 和 T 是**相关**的；  
    3. 若限定集为{W,Y}时,X和T条件独立。因为X和T之间只有有一条路径,且这条路径含有一个分连结构Z←Y→S,且Y在限定集{W,Y}中,因此Y阻塞了X和T之间的唯一路径,X和T是有向分离,即限定集为{W,Y}时,X和T条件独立,。  
    ![](png/Pasted%20image%2020251007225739.png)  
???+ tip "题目"    
    - **选项 A**：有向边 $T \to Y$，表示 T 对 Y 存在因果影响，属于因果图模型。  
    - **选项 B**：有向边 $Y \to T$，表示 Y 对 T 存在因果影响，属于因果图模型。  
    - **选项 C**：有向边 $Y \to T$，表示 Y 对 T 存在因果影响，属于因果图模型。  
    - **选项 D**：T 和 Y 之间没有任何有向边，无法体现变量间的因果关系，因此**不属于**因果图模型。  
    ![](png/Pasted%20image%2020251007225909.png)

#### 因果反事实模型  
- 干预：固定系统中的变量，然后改变系统，观察其他变量的变化
- 引入了 do 算子（并表示固定不变，而**不是条件概率**）

---
**因果效应差**  
![](png/Pasted%20image%2020251007230425.png)

$$
Pm(Y=y|X=x,Z=z)=P(Y=y|X=x,Z=z),Pm(Z=z)=P(Z=z)
$$

- 在操纵图中，X 和 Z 是 D 分离的，可以得到因果消音的表达式：

$$
\begin{aligned}P(Y=y|&do(X=x)):\\&P(Y=y|do(X=x))=P_{m}(Y=y|X=x)\\&=\sum_{z}P_{m}(Y=y|X=x,Z=z)P_{m}(Z=z|X=x)\\&=\sum_{z}P_{m}(Y=y|X=x,Z=z)P_{m}(Z=z)\end{aligned}
$$

得到最后的使用**正常的条件表示**的因果效应：  

$$
P(Y=y|do(X=x))=\sum_{z}P(Y=y|X=x,Z=z)P(Z=z)
$$

右端只包含正常的情况下的概率  
???+ tip "因果效应"  
    定理2.8(因果效应) 给定因果图G,PA表示X的父节点集合,!则X对Y的因果效应为:  

    $$  
    P(Y=y|do(X=x))=\sum_{z}P(Y=y|X=x,PA=z)P(PA=z)  
    $$

---
**反事实模型**  
反事实描述的是个体的行为  
???+ tip "计算的三个步骤"  
    - (1) 溯因 (abduction)：利用现有的证据 E 确定环境 U；  
    - (2) 动作 (action)：对模型 M 进行修改，移除等式 X 中的变量并将其替换为 X = x，得到修正模型 Mx；  
    - (3) 预测 (prediction)：利用修正模型 Mx 和环境 U 计算反事实 Yₓ(U) 的值。

???+ tip "计算干预为用药病人的因果效应"  
    ![](png/Pasted%20image%2020251007232114.png)

## 搜索求解  
### 搜索算法基础  
找到最短的路线搜索  
???+ tip "概念"  
    - 状态：对当前情形的描述，刚开始的状态称为**初始状态**  
    - 动作：从当前时刻所处的状态转移到下一时刻所处的状态所做的**操作**  
    - 状态**转移**：智能体选择一个动作之后，所处的状态发生的响应的变化，有边的两点之间才能转移  
    - 路径/代价：一个状态序列：该状态序列被一系列的操作所连接  
    - 目标**测试**：评估**当前状态**是否是所求解的目标状态

每次解决的时候，算法会从已经探索过的路径中选择一条，在末尾进行一次状态转移，可以写成搜索树的形式  
![](png/Pasted%20image%2020251009143338.png)  
???+ tip "说明"  
    - 一个城市，即三个**标号**为 A 的**结点**所对应**状态**相同，但是这三个节点在搜索树中却是不同结点，因为它们分别代表了从初始状态出发到达城市 A 的三条不同路径。  
    - 这三个结点表示的路径分别为：A → B → A、A → D → A 和 A → E → A。  
    - 因此需要注意的是，在搜索树中，**同一个标号一定表示相同的状态**，其含义为智能体当前所在的城市，但是**一个标号可能有多个的结点与之对应，不同的结点对应了从初始状态出发的不同路径**。  
    - 搜索算法可以被看成是一个构建搜索树的过程，从根结点（初始状态）开始，不断展开每个结点的后继结点，直到某个结点通过了目标测试。

#### 常见的搜索算法的评价指标  

|  特性   |          说明           |
| :---: | :-------------------: |
|  完备性  | 当问题存在解时，算法是否能保证找到一个解。 |
|  最优性  | 搜索算法是否能保证找到的第一个解是最优解。 |
| 时间复杂度 |      找到一个解所需时间。       |
| 空间复杂度 |  在算法的运行过程中需要消耗的内存量。   |  

完备性和最优性刻画了找到解的能力以及质量，时间复杂度空间复杂度衡量了算法的资源消耗，常用 O 来描述  

在**树搜索算法**中，集合 F 用于保存搜索树中用于下一步搜索的所有候选节点，这个集合称为**边缘集合**，有时叫做**开表**  
![](png/Pasted%20image%2020251009144326.png)  

剪枝搜索：不是所有的后继结点都值得被搜索  
图搜索：不允许环路的存在  
![](png/Pasted%20image%2020251009144941.png)  
通过剪枝的方式**防止闭环**存在  
???+ example "课后题"  
    - 1  
        - 广度优先搜索**按层扩展节点**。首先扩展初始节点 A（扩展顺序 1）。  
        - 然后扩展 A 的所有子节点 B、C、D（扩展顺序 2、3、4）。  
        - 接着扩展 B 的子节点 F（扩展顺序 5），C 的子节点 G（扩展顺序 6），D 的子节点 E（扩展顺序 7）。  
        - 此时，在扩展 F 时，发现 F 有到 I 的边，找到路径 $A \to B \to F \to I$，算法终止。  
    - 2  
        - 深度优先搜索优先扩展深度大的节点。首先扩展 A（扩展顺序 1）。  
        - 选择 A 的子节点中字典序较小的 B 扩展（扩展顺序 2）。  
        - 扩展 B 的子节点 F（扩展顺序 3）。  
        - 扩展 F 的子节点 I（扩展顺序 4），此时找到从 A 到 I 的路径，算法终止。  
    ![](png/Pasted%20image%2020251009145045.png)  

### 启发式搜索  
也叫有信息搜索  
利用与所搜索的信息相关的信息  

**贪婪最佳优先搜索**：评价函数=启发函数的搜索算法  
贪婪最佳优先搜索算法时间和空间复杂度均为 O (bᵐ)，b 是搜索树**分支因子**，m 是**最大深度**  

---
 $A^*$ 算法：  
评价函数：  

$$
\mathbf{f}(n)=\mathbf{g}(n)+\mathbf{h}(n)
$$

- 代价函数 g 表示从起始到节点 n 的开销代价
- 启发函数 h 表示从节点 n 到目标节点的路径中所估算的最小开销代价值（之后的）
- 评价函数 f 为经过节点 n 具有最小开销代价值的路径（已经有的）

![](png/Pasted%20image%2020251009150812.png)  

???+ tip "课后题"  
    - A 有子节点 B（代价 1）、C（代价 3）、D（代价 3）。  
        - 对于 B：$g(B) = 0 + 1 = 1$，$h(B) = 4$，$f(B) = 1 + 4 = 5$。  
        - 对于 C：$g(C) = 0 + 3 = 3$，$h(C) = 3$，$f(C) = 3 + 3 = 6$。  
        - 对于 D：$g(D) = 0 + 3 = 3$，$h(D) = 2$，$f(D) = 3 + 2 = 5$。  
    - 待扩展队列中的节点按 f 值排序，B、D（$f = 5$），C（$f = 6$）。由于 B 路径字典序**更小**，优先扩展 B。  
    - B 有子节点 F（代价 2）。  
        - 对于 F：$g(F) = 1 + 2 = 3$，$h(F) = 5$，$f(F) = 3 + 5 = 8$。  
    - 待扩展队列中的节点：D（$f = 5$）、C（$f = 6$）、F（$f = 8$）。优先扩展 D。  
    - D 有子节点 E（代价 1）。  
        - 对于 E：$g(E) = 3 + 1 = 4$，$h(E) = 5$，$f(E) = 4 + 5 = 9$。  
    - 待扩展队列中的节点：C（$f = 6$）、F（$f = 8$）、E（$f = 9$）。优先扩展 C。  
    - C 有子节点 G（代价 1）、H（代价 2）。  
        - 对于 G：$g(G) = 3 + 1 = 4$，$h(G) = 2$，$f(G) = 4 + 2 = 6$。  
        - 对于 H：$g(H) = 3 + 2 = 5$，$h(H) = 1$，$f(H) = 5 + 1 = 6$。  
    - 待扩展队列中的节点：G、H（$f = 6$），F（$f = 8$），E（$f = 9$）。G 路径字典序更小，优先扩展 G。  
    - G 有子节点 I（代价 3）。  
        - $g(I) = 4 + 3 = 7$，$h(I) = 0$，$f(I) = 7 + 0 = 7$。  
    - 此时找到终止节点 I，算法终止。  
    - 路径为 $A \to C \to G \to I$，总代价 $g(I) = 7$。  
    ![](png/Pasted%20image%2020251009151045.png)  

### 贪婪最佳优先搜索  
- **深度优先搜索**：沿着**一条路径**尽可能深地探索，直到无法继续或者达到目标节点，然后回溯到上一个未完全探索的节点，继续探索其他分支。可以想象成在迷宫中，每次都选择一条路走到尽头，不行再回头换路。
- **广度优先搜索**：逐层地探索节点，先访问起始节点的所有邻居节点，然后再访问这些邻居节点的邻居节点，以此类推，就像在池塘中扔一块石头，波纹从中心向四周一层一层扩散。

---
最短路径的问题  
已知每个节点的相邻节点及其**距离**  
辅助信息：每个节点与目标节点间的直线距离（启发函数）  

用更快到达目标节点的节点作为下一步的选择  
不过这条路径不一定是最优的路径  
这里将启发函数作为评价函数

---

#### 启发函数与评价函数  
贪婪最佳优先搜索 (Greedy best-first search): 评价函数 f (n)（决定下一步**选择的节点**）= 启发函数 h (n)  

辅助信息（启发函数）：任意城市与终点城市 K 之间的直线距离，由于启发函数就是评价函数，所以只看这个  

要将已经发生的历史与未来相结合  

???+ tip "对这个的性能分析："  
    - 完备性：（排除环路）完备，（有环路）不完备  
    - 最优性：**非最优**  
    - 时间复杂度：O (b<sup>m</sup>)  
    - 空间复杂度：O (b<sup>m</sup>)

### 对抗搜索  
零和博弈：一方的收益意味着另一个的损失，不能合作的  
![](png/Pasted%20image%2020251009153701.png)  
在两人的零和博弈时，两人的得分之后是固定的  
所以要最大化自己的得分，也就是要最小化对方的分数  
![](png/Pasted%20image%2020251009154352.png)  

---
在 Minimax（一方使分数最大，一方使得分数最小） 算法中可使用**剪枝搜索算法**减少被搜索的节点数  

剪枝不影响搜索的结果  
![](png/Pasted%20image%2020251009154713.png)  
???+ tip "分析"  
    - 假设有一个位于 MIN 层的结点 m，已知该结点能够向**其上 MAX** 结点**反馈的收益为 α**（alpha）。n 是与结点 m 位于同一层的某个兄弟（sibling）结点的后代结点。如果在结点 n 的后代结点被访问一部分后，知道结点 n 能够向其上一层 MAX 结点反馈收益小于 α，则**结点 n 的未被访问孩子结点将被剪枝**。  
    - 考虑位于 MAX 层的结点 m，已知结点 m 能够从其下 MIN 层结点收到的收益为 β（beta）。结点 n 是结点 m 上层结点 m' 的位于 MAX 层的后代结点，如果目前已知结点 n 能够收到的收益大于 β，则不再扩展结点 n 的未被访问后继结点，因为位于 MIN 层的结点 m' **只会选择收益小于或等于 β 的结点**来采取行动。

???+ tip "算法的步骤"  
    - **根结点（MAX 结点）的 α 值和 β 值**分别被初始化为 -∞和 +∞。  
    - 子节点继承父节点的 α 值和 β 值，再按照以下规则进行更新：  
    - 对于 **MAX 结点**，如果其孩子结点（MIN 结点）的收益大于当前的 α 值（极大层的下界），则将 α 值更新为该收益；对于 **MIN 结点**，如果其孩子结点（MAX 结点）的收益**小于**当前的 β 值（极小层的上界），则将 β 值更新为该收益。  
    - 随着搜索算法不断被执行，每个结点的 α 值和 β 值不断被更新。大体来说，每个结点的 [α,β] 从其父结点提供的初始值开始，取值按照如下形式变化：α 逐渐增加、β 逐渐减少。不难验证，如果一个结点的 **α 值和 β 值满足 α > β 的条件，则该结点尚未被访问的后续结点就可以被剪枝**（结合前面两个引理加以说明）。  
    - 实际上也是从下往上，从左往右推的  
        - 先最左边的，最下的两层，可以推出一个，再直接由这个节点再上一层，推出上一层的范围；之后再在同样的两层推理，此时这两层是  
        - 继承者上一层的范围的，就可以做出取舍  
        - 以此类推，及时更新

???+ info "课后习题"  
    ![](png/Pasted%20image%2020251009185140.png)

为什么是这么画的？
```Python
 MAX
 / \ 
MIN MIN 
/ \ / \ 
3 5 2 4
```

- 首先评估叶子节点，值分别为 3、5、2、4 。
- 然后计算 MIN 节点（**取子节点**的最小的）的值，左侧 MIN 节点的子节点值为 3 和 5，取最小值 3；右侧 MIN 节点的子节点值为 2 和 4，取最小值 2 。
- 最后计算 MAX 节点的值，它的子节点（两个 MIN 节点）值为 3 和 2，取最大值 3 。所以对于 MAX 方来说，选择**左侧 MIN 节点对应的决策就是最优决策**。
- 所以可以看出是**从下往上**倒着推的，max 和 min 是要决定**选择哪个子节点**

### 蒙特卡树搜索  
- 状态：
- 动作：
- 状态转移：
- 奖励：假设从第i个赌博机获得收益分数的分布为$D_i$，其均值为$\mu_i$。如果智能体在第t次行动中选择转动了第$l_t$个赌博机臂膀，那么智能体在第t次行动中所得收益分数$\hat{r}_t$ 服从分布$D_{l_t}$，$\hat{r}_t$被称为第t次行动的奖励。为了方便对多臂赌博机问题的理论研究，一般假定奖励是有界的，进一步可假设奖励的取值范围为$[0,1]$。
- 悔值函数：根据智能体的前 T 次动作，可以定义如下的悔值函数

$$
\rho_T=T\mu^*-\sum_{t=1}^T\hat{r}_t
$$

- 贪心算法：利用已有的尝试中得到的估计来指导后续的动作，需要用机制来平衡算法

---
- $\varepsilon$ 贪心算法

    $$
    l_t=\begin{cases}\operatorname{argmax}_i\bar{x}_{i,T_{(i,t-1)}},&\text{以}1-\epsilon\text{的概率}\\\text{随机的}i\in\{1,2,\cdots,K\},&\text{以}\epsilon\text{的概率}&\end{cases}
    $$

    即以 1-E 的概率选择在过去 t-1 次摇动赌博机臂膀行动中所得平均收益分数最高的赌博机进行摇动；以 E 的概率随机选择一个赌博机进行摇动

---
总结上面的想法：优先估计不确定度大的，但是对于奖励极端偏小的，不应该多计算  

---

#### 上限置信区间算法
为每个动作的奖励期望计算一个估计范围，优先采用**估计范围上限高**的动作  
![](png/Pasted%20image%2020251009194017.png)  
如图，优先计算动作 2（1 的奖励太小，3 的不确定度小）  

根据霍夫丁不等式  
期望的上限为  

$$
\overline{x}_{i,T_{(i,t-1)}}+C\sqrt{\frac{2\ln t}{T_{(i,t-1)}}}
$$

在 t 次时选择使得式子取值最大的动作  

$$
l_t=\operatorname{argmax}_i\bar{x}_{i,T_{(i,t-1)}}+C\sqrt{\frac{2\ln t}{T_{(i,t-1)}}}
$$

算法起初是不知道选择节点的奖励的，只能通过选择的均值来决定选择扩展哪些节点，放弃哪些节点等等  

???+ tip "步骤"  
    - 选择：选择指算法从搜索树的**根节点**开始，向下递归选择子节点，直至到达叶子节点或者到达具有还未被扩展过的子节点的节点L。这个向下递归选择过程可由$\text{UCB1}$算法来实现，在递归选择过程中**记录下每个节点被选择次数和每个节点得到的奖励均值**。  
    - 扩展：如果节点L不是一个终止节点（或对抗搜索的终局节点），则随机扩展它的一个**未被扩展过的后继边缘节点M**。  
    - 模拟：从节点M出发，**模拟扩展搜索树**，直到**找到一个终止节点**。模拟过程使用的策略和采用$\text{UCB1}$算法实现的选择过程并不相同，前者通常会使用比较简单的策略，例如**使用随机策略**  
    - 反向传播：用模拟所得结果（终止节点的代价或游戏终局分数）**回溯更新**模拟路径中M以上（含M）节点的奖励均值和被访问次数。  

???+ tip "课后题"  
    ![](png/Pasted%20image%2020251009200053.png)  

    - **选项 A**：蒙特卡洛树搜索的选择过程（如使用 UCB 公式），在利用已有信息（选择高价值节点）和探索新节点之间取得平衡，该说法正确。
    - **选项 B**：算法进入扩展步骤时，当前节点可能有部分子节点已被扩展，并非所有子节点必然都未被扩展，该说法错误。
    - **选项 C**：模拟步骤可以采用随机策略等，和选择步骤的策略（如基于 UCB）不一定相同，该说法正确。
    - **选项 D**：反向传播时，通常只需要更新当前模拟路径上已被扩展的节点的统计信息，该说法正确。  

## 机器学习——监督学习  
### 基本概念  
???+ tip "分类"  
    - 监督学习：数据有标签、一般为**回归或者是分类**等任务  
    - 无监督学习：数据无标签，一般为聚类或者是若干降维任务  
    - 强化学习：一般为从环境交互中学习，在不同的状态下选择什么样子的方式

- 分类任务中，监督学习的输出通常是**名义变量**
- 回归任务中，输出通常是真实数值

---

#### 分类问题  
- 标注数据
- 学习模型
- 损失函数

映射函数——预测的函数；**损失函数**是真实值与预测值之间的**差值**  

|      损失函数名称       |                                   损失函数定义                                    |
| :---------------: | :-------------------------------------------------------------------------: |
|    0 - 1 损失函数     | $Loss(y_i,f(x_i))=\begin{cases}1,f(x_i)\neq y_i\\0,f(x_i)=y_i\end{cases}$ |
|      平方损失函数       |                    $Loss(y_i,f(x_i))=(y_i - f(x_i))^2$                    |
|      绝对损失函数       |                $Loss(y_i,f(x_i))=\vert y_i - f(x_i)\vert$                 |
| 对数损失函数 / 对数似然损失函数 |            $Loss(y_i,P(y_i\vert x_i))=-\log P((y_i\vert x_i))$            |

从标签数据出发，得到一个使得损失函数最小的映射函数  

???+ tip "风险"  
    - 经验风险：  
        在**训练集**中产生的损失，（在训练集上面的表现）  
    - 期望风险：  
        **测试集**中存在无穷多数据时产生的损失（在测试集上的表现）  
    ![](png/Pasted%20image%2020251022113334.png)  
在最小化经验风险和防止过拟合之间寻找一个平衡  

$$
\min_{f\in\Phi}\frac{1}{n}\sum_{i=1}^nLoss(y_i,f(x_i))\quad+\quad\lambda J(f)
$$

前面的项为经验风险的计算，后面的为降低模型复杂度的修正  

#### 两种方法  
=== "判别方法"  
    - 直接学习判别函数或者条件概率  
    - 关心在给定的输入数据下，预测数据的输出是什么  
    - 回归模型、神经网络  
=== "生成方法"  
    - 学习联合概率分布  
    - 典型方法为贝叶斯方法、隐马尔科夫性  
    - 联合分布或者说是似然概率求取困难

???+ tip "例题"  
    1. 模型复杂度与误差的关系
    
    - 训练误差：模型复杂度越高，拟合训练数据的能力越强，训练误差**逐渐降低**。
    - 测试误差：模型复杂度较低时，测试误差随复杂度增加而降低；但当模型过拟合（复杂度过高）时，测试误差会**逐渐升高**，呈现 “先降后升” 的 U 型曲线。
    
     1. 数据集大小的影响：数据集 A（100K）比数据集 B（10K）大得多：
    
    - **训练误差**：数据集越小（B），模型越容易拟合训练集，因此在相同模型复杂度下，B 的训练误差会**低于 A 的训练误差**。
    - **测试误差**：数据集越大（A），包含的信息越丰富，模型泛化能力更强，因此 A 的测试误差会**低于 B 的测试误差**。
    
    1. 逐一分析选项
    
    - **图 A**：训练误差中 A.Train 低于 B.Train，与 “小数据集 B 更容易拟合，训练误差更低” 矛盾，排除。
    - **图 B**：横轴是 “模型复杂度”，但训练误差的规律符合 “B.Train < A.Train，A.Test < B.Test”，且测试误差随复杂度先降后升，符合预期。
    - **图 C**：测试误差中 A.Test > B.Test，与 “大数据集 A 泛化能力更强，测试误差更低” 矛盾，排除。
    - **图 D**：横轴是 “模型复杂度”，但训练误差的规律错误（A.Train < B.Train），排除。
    
    综上，**正确答案是图 B**。  
    ![](png/Pasted%20image%2020251022114336.png)  

### 线性回归  
身高的回归分析  
回归模型中的参数求取方法  
???+ tip "一元线性回归模型的例子"  
    常见的气温温度与火灾影响面积的直线方程  
    $y=ax+b$ 使得得到的残差值最小  
    使得$\min_{a,b}\quad L(a,b)=\sum_{i=1}^n(y_i-a\times x_i-b)^2$  
    分别对 a 和 b 求偏导数  
    最后得到的最小二乘法的方程：  

    $$  
    a=\frac{\sum_{i=0}^{n}x_{i}y_{i}-n\overline{x}\overline{y}}{\sum_{i=1}^{n}x_{i}x_{i}-n\overline{x}^{2}}  
    $$

???+ tip "题目"  
    ![](png/Pasted%20image%2020251022203820.png)  
    1. 解释$\lambda Y^TY$起不到标准化作用  
        L₂正则化的核心是**对模型参数w添加惩罚项**，通过限制w的大小来降低模型复杂度、防止过拟合。  
        而$\lambda Y^TY$中，Y是标签向量（真实值），是**与模型参数w无关的常量**。对损失函数求关于w的偏导时，$\lambda Y^TY$的偏导为0，即该项不会对参数w的更新产生任何约束。  
        因此，$\lambda Y^TY$无法限制参数w的大小，不能起到 L₂正则化（标准化）“约束参数、防止过拟合” 的作用。  
    2. 解释第二小问  
        L₂正则化的惩罚项是$\lambda w^Tw$（$\lambda > 0$），其目的是**最小化损失时，迫使w的范数尽可能小**，从而简化模型。  
        若$\lambda < 0$，则惩罚项变为$\text{负数} \times w^Tw$。此时，为了最小化整个损失函数L，模型会倾向于**最大化w的范数**（因为负的惩罚项会让损失随w范数增大而减小）。这与 L₂正则化 “约束参数大小、防止过拟合” 的目标完全相反，会导致模型参数无限制增大，更容易过拟合，因此起不到标准化作用。

### 决策树  
银行是否借贷的决策树  
使用属性值对样本进行划分，使得得到的子样本都是同一类型的  

---
衡量样本划分结果的好坏的方式：  
使用**信息熵增益**  

$$
E(D)=-\sum_{k=1}^Kp_k\log_2p_k
$$

所有的概率的和为 1  
对划分之后的样本集的信息熵进行计算  
对子样本集分别进行计算，之后原样本集的信息熵为：  

$$
Gain(D,A)=Ent(D)-\sum_{i=1}^n\frac{|D_i|}{|D|}Ent(D_i)
$$

其中的 D 为样本集，A 为选择划分的属性，$D_i$ 为根据属性划分的子样本集，所以计算的公式为划分之后的信息熵乘以在总样本中的占比  

???+ tip "题目"  
    ![](png/Pasted%20image%2020251022205122.png)  
    信息熵是衡量数据 “混乱程度” 的指标。熵越高，说明数据中类别越混杂，**纯度越低**；熵越低，数据类别越单一，纯度越高。
    
    - 选项 A（纯度高）：与 “高信息熵” 的含义相反，排除。
    - 选项 B（纯度低）：高信息熵意味着数据混乱、类别混杂，即纯度低，符合定义。
    - 选项 C（有用）、D（无用）：信息熵本身是衡量混乱度的指标，不能直接判定属性 “有用” 或 “无用”（属性的有用性需结合划分前后的熵减程度，即信息增益来判断），因此这两个选项不贴切。
    - 选项 E（以上都不贴切）：因选项 B 贴切，故排除。

### 线性判别分析  
让同一类别的数据抱团  

- 定义：  
    是一种基于监督学习的降维方法，对于具有标签信息的高纬样本数据，利用其类别信息，将其线性投影在一个低维空间上，在低纬空间中同一类别的尽量靠近，不同类别的尽量远离

#### 二分类问题  

$$
y(x)=w^Tx
$$

矩阵  
投影的方向：  
降维的结果  
???+ tip "步骤"  
    1. 计算数据样本集中每个类别样本的均值  
    2. 计算类内散度矩阵$S_w$和类间散度矩阵$S_b$  
    3. 根据$S_w^{-1}S_bW = \lambda W$来求解$S_w^{-1}S_b$所对应前r个最大特征值所对应特征向量$(w_1, w_2, ..., w_r)$，构成矩阵W  
    4. 通过矩阵W将每个样本映射到低维空间，实现特征降维。

???+ tip "习题"  
    ![](png/Pasted%20image%2020251022210653.png)第 1 题  
    线性判别分析（LDA）的核心思想是**最大化类间方差与类内方差的比值**，以此找到最优的线性投影方向，使不同类别在投影后尽可能分离，同一类别尽可能紧凑。因此该说法**正确**，答案选 A。  
    第 2 题：线性判别分析（LDA）与主成分分析（PCA）的异同  
    相同点  
    - 两者都属于**线性降维方法**，通过线性变换将高维数据映射到低维空间。  
    - 都旨在简化数据结构，便于后续的分析（如分类、可视化等）。  
    不同点  

    |维度|线性判别分析（LDA）|主成分分析（PCA）|  
    |---|---|---|  
    |**目标**|最大化**类间区分度**，使不同类别在低维空间中尽可能分离|最大化**数据方差**，保留原始数据的大部分变异信息|  
    |**监督性**|有监督（利用类别标签信息）|无监督（不依赖类别标签，仅基于数据本身的方差）|  
    |**降维后维度上限**|最多为 “类别数 - 1”（如二分类问题，最多降维到 1 维）|最多为原始数据的维度数|  
    |**应用场景**|更适合**分类任务**，强调类别间的分离性|  

### Ada Boosting 算法  
弱分类器构建强分类器  
对于复杂的分类任务，分解为若干子任务，完成方法综合，最终完成复杂的任务  
???+ tip "核心问题"  
    1. 提高上一轮未被正确分类的样本在下一轮中的权重（改变训练数据的权重）  
    2. 提高分类的误差下的弱分类器在组合强分类器重的权重  
    （加权多数表决方法来提高分类误差小的弱分类器的权重）

#### 数据样本权重初始化  
1. 先将 n 个训练物体的权重均分  
2. 分类误差：
    1. 误差函数，为没有正确分类的误差的累计$err_{m}=\sum_{i=1}^{N}w_{m,i}I(G_{m}(x_{i})\neq y_{i})$
    2. 根据分类误差计算弱分类器的权重：
    3. 该分类器的**权重**为 $\alpha_{m}=\frac{1}{2}\mathrm{ln}\frac{1-err_{m}}{err_{m}}$
    4. 错误率为 1/2 的分类器的权重是 0，相当于随机分类器的行能
3. 更新下一个分类器分类时，某一个分类数据的权重
    1. 计算公式为：$D_{m+1}=w_{m+1,i}=\frac{w_{m,i}}{z_{m}}e^{-\alpha_{m}y_{i}G_{m}(x_{i})}$
    2. 在上一个弱分类器中错误的话，权重高一点

        $$
        w_{m+1,i}=\begin{cases}\frac{W_{m,i}}{Z_m}e^{-a_m},&G_m(x_i)=y_i\\\frac{W_{m,i}}{Z_m}e^{a_m},&G_m(x_i)\neq y_i&\end{cases}
        $$

1. 以**线性加权**的方式组合弱分类器

    $$
    f(x)=\sum_{i=1}^M\alpha_mG_m(x)
    $$

    得到的就是强分类器

    $$
    G(x)=sign(f(x))=sign(\sum_{i=1}^M\alpha_mG_m(x))
    $$

    决定正负样本的值

???+ tip "步骤"  
    1. 样本的权重初始化  
    2. 分别训练 M 个基分类器（弱分类器）  
    3. 当阈值为多少时，基分类器有最小的错误率  
    4. 直到所有的样本都已经被正确的分类（曾经被正确分类也算）  
???+ example "例题"  
    ![](png/Pasted%20image%2020251022213220.png)  
    - **选项 A**：若样本被第t轮的弱分类器错误分类，AdaBoosting 会增加其权重，使其在第$t+1$轮更受关注，该情况成立。  
    - **选项 B**：第t轮后的集成分类器（强分类器）若错误分类该样本，说明该样本 “难分类”，其权重会被增加，该情况成立。  
    - **选项 C**：若样本被到第t轮为止的大多数弱分类器错误分类，说明它是 “顽固” 的难分样本，权重会被增加，该情况成立  

## 无监督学习  
### k 均值聚类  
将相似的东西分类，学习一种映射函数  

---
- 监督学习：每一个数据上都标注上数据的类型（有一个标签）
- 无监督学习：是相似的，但是数据的**语义标签**是不知道的  
    所以对于得到的数据结果并不知道每一部分代表的语义  
    ![](png/Pasted%20image%2020251124211104.png)

通过**相似度**的函数衡量  
所以不同的相似度函数下聚类的结果是不一定相同的  

---
- k 均值聚类
    - n 个 m 维度的数据：$\{x_{1},x_{2},...,x_{n}\},x_{i}\in R^{m}(1\leq i\leq n)$
    - 两个 m 维度数据之间的欧式距离为

    $$
    d\left(x_{i},x_{j}\right)=\sqrt{(x_{i1}-x_{j1})^{2}+(x_{i2}-x_{j2})^{2}+\cdots+(x_{im}-x_{jm})^{2}}
    $$

    距离的值越小，两个数据之间的相似度越高
- 聚类几何数目 k（将 n 个数据分类到 k 个集合中去）

---
!!! note "k 均值算法的步骤"  
    1. 初始化聚类质心  
        - 初始化 k 个聚类质心  
        - 每个质心都在一个集合中  
    2. 将每个待聚类的数据放入唯一一个聚类集合中  
        - 计算待聚类的数据与不同质心的距离  
        - 将每个数据 $x_i$ 放入距离最小的质心所在的集合中  
    3. 根据聚类的结果，更新聚类的质心  
        - 根据聚类集合中的数据，**更新**集合质心值  
        - 即：$c_{j}=\frac{1}{|G_{j}|}\sum_{x_{i}\in G_{j}}x_{i}$  
    4. 算法迭代，重复上述的步骤，不断更新聚类的质心  
        - 迭代终止的条件：（**满足一个**就停止）  
            - 到达迭代的次数上限  
            - 前后的两次迭代中，聚类的质心基本上保持不变

---

#### 方差视角下的聚类均值  
第 i 个类的方差：  

$$
\begin{equation}
    \mathrm{var}(G_{i})=\frac{1}{|G_{i}|}\sum_{x\in G_{i}}||x-G_{i}||^{2}
\end{equation}
$$

所以使得每个聚类的方差最小  

---
???+ tip "不足"  
    1. 首先要确定**聚类的数目**，很多时候我们是不知道的  
    2. **初始化**聚类质心对聚类的结果有很大的影响  
    3. 算法是迭代进行，**时间开销非常大**  
    4. 欧式距离假设每个维度之间的重要性是一样的（实际上是有权重的）

#### 课后题  
???+ tip "可以从最小化每个类簇的方差这一视角来解释 K 均值聚类的结果，下面对这一视角描述不正确的是（）"  
    a) 最终聚类结果中每个聚类集合中所包含数据呈现出来差异性最小  
    b) 每个样本数据分别归属于与其距离最近的聚类质心所在聚类集合  
    c) 每个簇类的方差累加起来最小  
    d) 每个簇类的质心累加起来最小  
    
    ---
    - 选项分析
        - **选项 a**：K 均值聚类的目标是让同一簇内数据尽可能相似，“簇内数据差异性最小” 对应 “簇内方差最小”，描述正确。
        - **选项 b**：K 均值的迭代步骤之一是 “将样本分配给距离最近的质心所在簇”，这一操作是为了缩小簇内方差，描述正确。
        - **选项 c**：K 均值的核心目标就是 “所有簇的方差之和最小”，这是其优化的目标函数，描述正确。
        - **选项 d**：K 均值关注的是 “簇内方差”，而非 “质心的累加值”（质心是簇内数据的均值，其累加无实际意义），描述错误。  

### 主成分分析  
是一种特征降维的方法  

---
人脑在识别图像的时候，会化繁为简，过滤出主要的特征进行分析  
将高维的数据降维，同时保持原有的特征不变  

#### 方差协方差和相关系数  
**方差**：  

$$
\begin{equation}
    \mathrm{Var}(\mathbf{X})=\frac{1}{n}\sum_{i=1}^n(x_i-\boldsymbol{u})^2
\end{equation}
$$

- 与均值之差的平方和的均值

---
**协方差**：  

- 衡量**两个变量**的相关度

    $$
    \begin{equation}
        \mathrm{cov}(\mathbf{X},\mathbf{Y})=\frac{1}{n}\sum_{i=1}^n(x_i-E(X))(y_i-E(Y))
    \end{equation}
    $$

- 例子：
- ![](png/Pasted%20image%2020251130233417.png)

---
- 协方差<0，为负相关

- >0，为正相关

---
**皮尔逊相关系数**：  

$$
\begin{equation}
    \mathbf{corr}(\mathbf{X},\mathbf{Y})=\frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}=\frac{Cov(X,Y)}{\sigma_x\sigma_y}
\end{equation}
$$

将两组变量之间的关联度规整到**一定的取值范围**内（就是线性回归方程那个公式）  
???+ tip "具有的性质"  
    - $|\text{corr}(X,Y)| \leq 1$  
    - $\text{corr}(X,Y) = 1$的充要条件是存在常数a和b, 使得$Y = aX + b$  
    - 皮尔逊相关系数是对称的，即$\text{corr}(X,Y) = \text{corr}(Y,X)$  
    - 由此衍生出如下性质：皮尔逊相关系数刻画了变量X和Y之间线性相关程度，如果$|\text{corr}(X,Y)|$的取值越大，则两者在线性相关的意义下相关程度越大。$|\text{corr}(X,Y)| = 0$表示两者不存在线性相关关系（可能存在其他非线性相关的关系）。

- 独立一定不相关
- 不相关不一定相互独立（不相关更弱）
- 独立没有任何的关系，相关是线性关系

---
???+ tip "例题"  
    下面对相关性 (correlation) 和独立性 (independence) 描述不正确的是（）  
    a) 如果两维变量线性不相关，则皮尔逊相关系数等于 0  
    b) 如果两维变量彼此独立，则皮尔逊相关系数等于 0  
    c) “不相关” 是一个比 “独立” 要强的概念，即不相关一定相互独立  
    d) 独立指两个变量彼此之间不相互影响
    
    ---
    - **选项 a**：线性不相关的定义就是皮尔逊相关系数为 0，描述正确。
    - **选项 b**：若变量独立，则它们的线性相关程度必然为 0（独立意味着没有任何关联，包括线性关联），描述正确。
    - **选项 c**：实际逻辑是 “独立” 是比 “不相关” 更强的概念 ——**独立一定不相关，但不相关不一定独立**（比如变量存在非线性关联时，可能不相关但不独立），因此 “不相关一定相互独立” 的描述错误。
    - **选项 d**：独立性的核心含义就是变量之间互不影响，描述正确。

### 主成分分析  
是一种特征降维的方法：保持数据投影之后的**方差最大**（保持原有的特征）  
![](png/Pasted%20image%2020251201233513.png)  
图中，向 y 方向投影之后的方差大，特征没有削减太多，这样就变成一维的了  
![](png/Pasted%20image%2020251201233559.png)  
但是还是向中间的黄线投影最好  

---
- 主成分分析的核心思想：将 n 维特征数据映射到 l 维空间（n 远大于 l），通过**去除数据间的相关性**，消除原始数据的冗余性。  
???+ tip "步骤"
    1. **数据定义**
        - 样本集合：$D = \{x_1, x_2, ..., x_n\}$（共n个d维样本，$x_i \in R^d$）
        - 数据矩阵：集合D表示为$n \times d$的矩阵$\boldsymbol{X}$
    2. **数据预处理**
        - 假设各维度特征均值已标准化为 0
    3. **映射目标**
        - 求取$d \times l$的映射矩阵$\boldsymbol{W}$（$l < d$）
    4. **降维映射**
        - 单个样本映射：$x_i$（$1 \times d$）经$\boldsymbol{W}$映射到l维空间，即$(x_i)_{1 \times d} (\boldsymbol{W})_{d \times l}$
        - 整体降维结果：降维后数据$\boldsymbol{Y} = \boldsymbol{X} \boldsymbol{W}$（$\boldsymbol{Y}$为$n \times l$矩阵）  

要使得转化之后的方差最大：$\mathbf{Y}=n\times l\quad\mathbf{X}=n\times d\quad\mathbf{W}=d\times l$  
样本空间存在很大的冗余，所以要去除相关性。  
所以映射矩阵的求法是：  

$$
\Sigma\mathbf{w}_{i}=\lambda_{i}\mathbf{w}_{i},\text{且}trace(\mathbf{W}^{T}\mathbf{\Sigma}\mathbf{W})=\sum_{i=1}^{l}\mathbf{w}_{i}^{T}\mathbf{\Sigma}\mathbf{w}_{i}=\sum_{i=1}^{l}\lambda_{i}
$$

???+ tip "算法步骤"
- ![](png/Pasted%20image%2020251201234604.png)
- 步骤：
1. 对于每个样本数据$\boldsymbol{x}_i$进行中心化处理：  
    $\boldsymbol{x}_i = \boldsymbol{x}_i - \mu, \quad \mu = \frac{1}{n}\sum_{j=1}^{n}\boldsymbol{x}_j$
2. 计算原始样本数据的协方差矩阵：  
    $\boldsymbol{\Sigma} = \frac{1}{n-1}\boldsymbol{X}^{\text{T}}\boldsymbol{X}$
3. 对协方差矩阵$\boldsymbol{\Sigma}$进行特征值分解，对所得特征根按其值大到小排序$\lambda_1 \geq \lambda_2 \geq \dots \geq \lambda_d$
4. 取前l个最大特征根所对应特征向量$\boldsymbol{w}_1,\boldsymbol{w}_2,\dots,\boldsymbol{w}_l$组成映射矩阵$\boldsymbol{W}$
5. 将每个样本数据$\boldsymbol{x}_i$按照如下方法降维：$(\boldsymbol{x}_i)_{1 \times d}(\boldsymbol{W})_{d \times l}=1 \times l$

???+ tip "题目"  
    ![](png/Pasted%20image%2020251201234741.png)  
    
    ---
    - **选项 a**：主成分分析核心是将高维特征映射到低维空间，属于特征降维方法，描述正确。
    - **选项 b**：主成分分析的目标是让投影后的方差最大（保留最多信息），描述正确。
    - **选项 c**：向方差最大方向投影是为了保留更多信息，让数据差异更明显（“彰显个性”），描述正确。
    - **选项 d**：主成分分析的结果是低维数据各维度之间**不相关**（通过特征向量正交性实现），并非 “极大相关度”，描述错误。  

