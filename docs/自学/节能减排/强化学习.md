## 学习的内容  
- RL 核心概念：智能体（Agent）、环境（Environment）、动作（Action）、状态（State）、奖励函数（Reward Function）、马尔可夫决策过程（MDP）。
- 经典算法：先掌握 DQN、PPO（最适合车辆控制的入门算法），理解策略梯度、价值函数的基本原理。
- Python RL 框架：重点学 **Stable Baselines3** 或 **PyTorch Lightning**，前者封装度高适合快速上手，后者灵活性强适合深度定制。

## 基本概念  
![](png/Pasted%20image%2020260206184806.png)  

找到最好的 Function 使得动作得到的收益（reward）最大  

---

### 例子  
1. 一个小游戏  
    ![](png/Pasted%20image%2020260206185229.png)  

    - **输入**就是 observation（观察），在这里就是游戏此刻的画面
    - 动作 action（**输出**）为左右移动或者开火
    - 最后使得得到的收益最大
2. 围棋游戏：  
    ![](png/Pasted%20image%2020260206185435.png)
    1. 环境给一个棋盘图像（或者说棋局的情况），作为输入，动作就是所有未下的地方
    2. 动作（输出）会使得环境变化，产生新的输入
    3. 这里的最终的奖励不会随着每一步的动作改变，而是最终游戏结束时才会产生：  
        ![](png/Pasted%20image%2020260206185714.png)

---

### 强化学习和机器学习  
机器学习的一般步骤：  

1. 定义一个带有未知数的 function
2. 定义一个损失 function
3. 找出使得 loss 最小化的未知数

我们的**强化学习**也是一样的  

1. 首先是 function with unknown
    1. 输入：例像素画面
    2. actor（policy network）（网络，在加入了深度强化学习之后）（这种网络的结构需要设计）
    3. 输出行动（采用几率的方式，引入随机性（sample），是非常重要的）  
    ![](png/Pasted%20image%2020260206190409.png)
2. 定义 loss
    1. 从开始到结束的过程，每一步可能都会得到 reward，最后得到的总的奖励为（return——reward 的总和）：$R=\sum_{t=1}^Tr_t$
    2. 在 RL 的情景下，将-R 看做我们的 loss，使得这个 loss 最小，即 reward 最大
3. optimization：
    1. ![](png/Pasted%20image%2020260206191002.png)  
        这个互动的过程的集合wield 为：$\tau=\{s_1,a_1,s_2,a_2,\cdots\}$
    2. ![](png/Pasted%20image%2020260206191146.png)
    3. 找出一个 network （actor）的参数，使得 $R(\tau)=\sum_{t=1}^Tr_t$ 大  
        有几个点，首先对于相同的输入，产生的输出是随机的  
        网络 network 是黑盒，环境也是黑盒  
        环境和 reward 有随机性

### 常用的算法 policy gradient  
我们怎么操控 actor 的输出  
类似于一个分类的问题  

![](png/Pasted%20image%2020260206192352.png)  

$$
L=e_1-e_2\quad\theta^*=arg\min_\theta L
$$

学习得到网络的**参数**，使得采取的动作与**标签**（$\widehat{a}$）（训练的资料）产生的差距最小  

---
推广到很多的训练集上  

![](png/Pasted%20image%2020260206192756.png)  

也就是加权一下：$L=\sum{A}_ne_n$  

## 如何控制 actor  
所以参数的选择就是要：$\theta^*=arg\min_\theta L$，使得 loss 最小，使得得到的奖励最大

### 版本 0
---
收集一些训练资料

1. 一个 actor 与环境进行互动（可以是一个随机的 actor，也可以是一些已经有的数据），将其得到的**结果**记录下来：  
    ![](png/Pasted%20image%2020260206220906.png)
2. 那么某个动作对应的加权是什么呢：
    1. 可以先将每一步得到的收益作为系数：![](png/Pasted%20image%2020260206221038.png)  
        但是，这一一个**局部最优**的做法，因为每一个行为都会影响到接下来发生的事情（可能会需要牺牲短期的利益，达到长期的目标）

---

### 版本 1
所以我们需要真正学习如何去确定某个动作的参数：  

![](png/Pasted%20image%2020260206221541.png)  

> 将执行了 $a_1$ 之后所有的增益加起来就是其参数  
> 类推  

$$
\begin{aligned}&G_{1}=r_1+r_2+r_3+......+r_N\\&G_{2}=r_2+r_3+......+r_N\\&G_{3}=r_3+......+r_N\end{aligned}
$$

### 版本 3
---
**进一步优化**：（由于随着时间的推移，一步操作对后续操作的影响越来越小）  
优化后的公式为：  

$$
G_{1}^{\prime}=r_{1}+\gamma r_{2}+\gamma^{2}r_{3}+......\text{Discount factor }\gamma<1
$$

### 版本 4  
对奖赏要进行标准化：

**步骤**：  

1. 初始化 actor network 的 $\theta^{0}$
2. 进行训练 $i=1->T$：
    1. 使用 $\theta^{i-1}$ 去进行训练
    2. 训练得到数据为： $\{s_1,a_1\},\{s_2,a_2\},...,\{s_N,a_N\}$
    3. 计算参数 $A_1,A_2,...,A_N$
    4. 计算损失 $L$
    5. 进行参数的更新 $\theta^i\leftarrow\theta^{i-1}-\eta\nabla L$ (只能更新一次)
    6. 重复上面的步骤
3. 此时每次更新一次参数就需要收集一次资料
4. 我们可以使用 **PPO** 来实现不用每次都重新训练：  
    训练的actor 必须知道它的得到数据的 actor 是不一样的，具体怎么实现不知道
5. actor 动作必须要有一定的随机性，尽量全面一点

## actor-critic  
- critic：评估一个 actor 的**好坏**的性能（当一个 actor 输入一个 s 时（或者加上做一个动作 a）的条件下），获得的收益
- 我们使用的 value function：$V^{\theta}(s)$  
    当使用一个 **actor** $\theta$ 时，当输入一个 s 时，接下来得到的 reward 是多少：$G_1^{\prime}=r_1+\gamma r_2+\gamma^2r_3+......$  
    而这里的 $V^{\theta}(s)$，就是一个**预测**的作用（未卜先知），就是看到某个游戏画面之后，预测得到的 reward（因为上面的公式是游戏结束后才能完全知道的）  
    $V^{\theta}(s)$ 的数值与 actor 有关

---
为了得到上面的那些值，有哪些方法：  

- MC  
    actor 与环境互动，互动到**游戏结束**  
    ![](png/Pasted%20image%2020260207223253.png)  
    也就是直接使用训练的资料来得到对应的 $V^{\theta}(s)$
- TD：  
    **不需要完全结束**，只需要下面的资料：$\cdots s_t,a_t,r_t,s_{t+1}\cdots$
    
    $$
    \begin{aligned}&V^{\theta}(s_{t})=r_{t}+\gamma r_{t+1}+\gamma^{2}r_{t+2}\ldots\\&V^{\theta}(s_{t+1})=r_{t+1}+\gamma r_{t+2}+\cdots\\&V^{\theta}(s_{t})=\gamma V^{\theta}(s_{t+1})+r_{t}\end{aligned}
    $$

> 上面就是一个递推的公式，我们需要训练得到的两个数值的差为：  
> ![](png/Pasted%20image%2020260207223710.png)  
> 差和 $r_t$ 接近即可  

---
上述两种方法的**比较**：（还有如何使用上述的方法）  

![](png/Pasted%20image%2020260207224106.png)  

> 其中的 $V^\theta(s_b)=3/4$，两种方法都是 0.75（八次的和除以八）  
> 但是 $V^\theta(s_a)=?$，两种方法得到的数值是不一样的  
> MC 的话，值为 0 (只看**第一次游戏**，认为 a, b 两者是相关的，a 之后会影响 b)  
> TD 的话，值为 $V^\theta(s_a)=V^\theta(s_b)+r\quad3/4\quad3/4\quad0$（认为 a, b 之间是不相关的）  

### 进一步的 A 值  
![](png/Pasted%20image%2020260207224642.png)  

要减去对应的 $V$  
为什么值是这样的：$\{s_t,a_t\}\quad A_t=G_t^{\prime}-V^\theta(s_t)$  

> $V^\theta(s_t)$ 的含义为：  
> ![](png/Pasted%20image%2020260207224939.png)  
> 是一个期望值（在看到某个游戏画面时），但是此时不一定会执行 $a_t$（因为是随机的），所以最后得到的奖励会有很多种，将所有可能的结果平均一下就是：$V^\theta(s_t)$  
> 而 $G_t^{'}$ 是一定会执行 $a_t$，得到的 reward 就是$G_t^{'}$  
> ![](png/Pasted%20image%2020260207225402.png)

- $\mathrm{A}_{t}>0$：说明执行 $a_t$ 比平均好
- 反之则没有平均好

---
但是上述的后一种只是**一种可能性**，会存在很大的误差：  
所以我们最终的版本为：  
![](png/Pasted%20image%2020260207225547.png)  

最后得到的 $A_t$ 的表达式为：  

$$
r_t+V^\theta(s_{t+1})-V^\theta(s_t)
$$

这就是我们的**actor-critic**  

---
设计的一个小技巧：  
![](png/Pasted%20image%2020260207225833.png)  

还有其他的方法，比如说 DQN  

## reward Shaping  
我们之前学到的是：  
![](png/Pasted%20image%2020260207230432.png)  
也就是 actor 与环境互动，得到收益，最后得到一个分数 $A_t$，作为训练的依据  

但是，当大多数情况下的 reward 都是 0 时，怎么解决呢（比如下围棋的时候），甚至像训练机械手臂打螺丝的时候，那就更加没有 reward  
所以我们需要**加入一些 reward**（reward shaping）(比如望梅止渴的故事)  

---
???+ tip "实际使用的例子"  
    怎么定义这个 reward  
    需要人为添加的  
    ![](png/Pasted%20image%2020260207232642.png)  

---
一个很有趣的 reward shaping 的做法——添加好奇心（机器在动作的时候得到有意义的新的东西就加分）——有意义的新  
但是有时候会面对一些杂讯（那就是无意义的新了）  
## No reward（完全没有奖励）  
我们找一些人类，也与环境进行互动  
![](png/Pasted%20image%2020260207233745.png)  
我们凭借这些示范（比如自动驾驶），与环境互动进行训练  

---
**这是不是一个 supervised learning**？  
比如自动驾驶中复制人类的行为，但是存在一个问题（人类和机器观察到的输入可能是不一样的）（人类的示范**不会很全面**）（同样也**不是所有的人类行为都需要模仿**）  

---
所以引入接下来的 inverse reinforcement learning（IRL）  
我们需要**机器从 expert 中学习得到 reward**：  
![](png/Pasted%20image%2020260207234530.png)  

- 原则：老师的行为是最棒的（不代表要完全模仿老师的行为）
- 基本的方法：
    - 初始化 actor
    - 在
        - actor 与环境互动
        - 学习定义 reward function，这个 function 要使得老师的得分高于 actor：$\sum_{n=1}^KR(\hat{\tau}_n)>\sum_{n=1}^KR(\tau)$
        - 更新 actor，使得 reward 最大化（使用新的 reward function）
    -  ![](png/Pasted%20image%2020260208102544.png)
    - 两种的比较（GAN 和 IRL）
        - ![](png/Pasted%20image%2020260208102742.png)
        - 两者有异曲同工之妙

---  
人为的演示  
给一个图像进行模仿  
